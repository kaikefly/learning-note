{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装\n",
    "\n",
    "Google 提供的 BERT 代码在[这里](https://www.github.com/google-research/bert)，可以直接 git clone 下来。运行它需要 Tensorflow 1.11 及其以上的版本。\n",
    "\n",
    "### 预训练模型\n",
    "\n",
    "Google 提供了预训练的模型（checkpoint），目前包括英语、汉语和多语言三类模型：英语有多个版本，汉语只有一个版本：BERT-Base,Chinese，包括简体和繁体汉字，共 12 层，768 个隐单元，12 个 Attention Head，110M 参数。另外一个多语言的版本是 BERT-Base, Multilingual Cased，它包括 104 种不同语言，12 层，768 个隐单元，12 个 Attention Head，110M 参数。所有这些模型的下载地址在[这里](https://www.github.com/google-research/bert#pre-trained-models)。每个模型 .zip 文件包含三个部分：\n",
    "* bert_model.ckpt：预训练权重（实际上是三个文件）\n",
    "* vocab.txt：映射 WordPiece 到 word id\n",
    "* bert_config.json：模型的超参（不需要修改）\n",
    "\n",
    "### 运行 Fine-Tuning\n",
    "\n",
    "对于大部分情况，不需要重新训练。我们要做的只是根据具体的任务进行 fine-tuning，这里以 GLUE 的 MRPC 为例子，我们首先需要下载预训练的模型然后解压，比如解压后的位置是：\n",
    "```\n",
    "export BERT_BASE_DIR=/home/model/uncased_L-12_H-768_A-12\n",
    "```\n",
    "\n",
    "接下来下载 GLUE 数据，可以使用这个[脚本](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)下载，可能需要代理。假设下载后的位置是：\n",
    "```\n",
    "export GLUE_DIR=/home/data/glue_data\n",
    "```\n",
    "\n",
    "GLUE 中 MRPC 任务的格式为：\n",
    "```\n",
    "index  #1 ID  #2 ID  #1 String  #2 String\n",
    "0  1089874  1089925  string1  string2\n",
    "1  3019446  3019327  string1  string2\n",
    "```\n",
    "\n",
    "数据是 tsv 文件，每行有 4 个用 Tab 分割的字段，分别表示 index，第一个句子的 id，第二个句子的 id，第一个句子，第二个句子。也就是输入两个句子，模型判断它们是否是同一个意思。如果是测试数据，那么第一列就是 index（无意义），如果是训练数据，那么第一列就是 0 或者 1，其中 0 表示不同，1 表示相同。接下来可以运行以下命令来进行 fine-tuning:\n",
    "```\n",
    "python run_classifier.py \\\n",
    "    --task_name=MRPC \\\n",
    "    --do_train=true \\\n",
    "    --do_eval=true \\\n",
    "    --data_dir=$GLUE_DIR/MRPC \\\n",
    "    --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n",
    "    --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n",
    "    --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n",
    "    --max_seq_length=128 \\\n",
    "    --train_batch_size=8 \\\n",
    "    --learning_rate=2e-5 \\\n",
    "    --num_train_epochs=3.0 \\\n",
    "    --output_dir=/tmp/mrpc_output/\n",
    "```\n",
    "\n",
    "简单介绍下参数的含义：\n",
    "* task_name：任务的名字\n",
    "* do_train：是否训练\n",
    "* do_eval：是否在训练结束后验证\n",
    "* data_dir：训练数据目录，配置了环境变量后不需要修改，否则填入绝对路径\n",
    "* vocab_file：BERT 模型的词典\n",
    "* bert_config_file：BERT 模型的配置文件\n",
    "* init_checkpoint：fine-tuning 的初始化参数\n",
    "* max_seq_length：token 序列的最大长度，这里是 128\n",
    "* train_batch_size：batch 大小，对于普通的 8GB 的 GPU，最大 batch 大小只能是 8，再大就会 OOM\n",
    "* learning_rate：学习率\n",
    "* num_train_epochs：训练的 epoch 次数，根据任务进行调整\n",
    "* output_dir：训练得到的模型的存放目录\n",
    "\n",
    "这里最常见的问题是内存不够，通常我们的 GPU 只有 8G 左右的显存，因此对于小的模型（bert-base），最多使用 batchsize=8，而如果要使用 bert-large，那么 batchsize 只能设置为 1。运行结束可能得到类似如下的结果：\n",
    "```\n",
    "***** Eval results *****\n",
    "eval_accuracy = 0.845588\n",
    "eval_loss = 0.505248\n",
    "global_step = 343\n",
    "loss = 0.505248\n",
    "```\n",
    "\n",
    "这说明验证集上的准确率为 0.84 左右。\n",
    "\n",
    "### DataProcessor\n",
    "\n",
    "DataProcessor 是一个抽象基类，定义了 get_train_examples、get_dev_examples、get_test_examples,、get_labels 等 4 个需要子类实现的方法，另外还提供了一个 _read_tsv 函数用于读取 tsv 文件。下面我们通过一个实现类 MrpcProcessor 来了解怎么实现这个抽象基类。如果想使用自己的数据，就需要自己实现一个子类。\n",
    "\n",
    "### MrpcProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE vesion)\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "    \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(line[3])\n",
    "            text_b = tokenization.convert_to_unicode(line[4])\n",
    "            if set_type == \"test\":\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                label = tokenization.convert_to_unicode(line[0])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是训练集和验证集，那么第一列 line[0] 就是真正的 label，而如果是测试集，label 没有意义，随便设为 \"0\"。然后对于所有的字符都使用 tokenization.convert_to_unicode 把字符串变成 unicode 的字符串。这是为了兼容 Python2 和 Python3，因为 Python3 的 str 就是 unicode，而 Python2 的 str 其实是 bytearray。\n",
    "\n",
    "最终构造出一个 InputExample 对象来，它有四个属性：guid、text_a、text_b、label。guid 只是个唯一的 id 而已，text_a 代表第一个句子，text_b 代表第二个句子，第二个句子可以为 None，label 代表类标签。\n",
    "\n",
    "### 分词\n",
    "\n",
    "如果想要把 BERT 产品化，我们需要使用 Tensorflow Serving，Tensorflow Serving 的输入是 Tensor，把原始输入变成 Tensor 一般需要在 Client 端完成。BERT 的分词是 Python 的代码，如果我们使用其它语言的 gRPC Client，那么需要用其它语言实现同样的分词算法，否则预测时会出现问题。\n",
    "\n",
    "#### FullTokenizer\n",
    "\n",
    "BERT 里分词主要是由 FullTokenizer 类来实现的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer(object):\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_tokens_to_ids(self.vocab, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FullTokenizer 的构造函数需要传入参数词典 vocab_file 和 do_lower_case。如果我们自己从头开始训练模型，那么 do_lower_case 决定了我们的模型是否区分大小写。如果只是 fine-tuning，这个参数需要与模型一致，比如模型是 uncased_L-12_H-768_A-12，那么 do_lower_case 就必须为 True。\n",
    "\n",
    "函数首先调用 load_vocab 加载词典，建立词到 id 的映射关系。接下来是构造 BasicTokenizer 和 WordpieceTokenizer，前者是根据空格等进行普通分词，而后者会把前者的结果再细粒度切分为 WordPiece。首先来看 BasicTokenizer 的 tokenize 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(self, text):\n",
    "    text = convert_to_unicode(text)\n",
    "    text = self._clean_text(text)\n",
    "    \n",
    "    # 这是2018年11月1日为了支持多语言和中文增加的代码。这个代码也可以用于英文模型，因为在英语的训练预料中基本不会出现中文字符。\n",
    "    text = self._tokenize_chinese_chars(text)\n",
    "    \n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "        if self.do_lower_case:\n",
    "            token = token.lower()\n",
    "            token = self._run_strip_accents(token)\n",
    "        split_tokens.extend(self._run_split_on_punc(token))\n",
    "        \n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先调用 convert_to_unicode 把输入变成 unicode，然后用 _clean_text 函数，它的作用是去除一些无意义的字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(self, text):\n",
    "    \"\"\"去除一些无意义的字符以及 whitespace\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cp = ord(char)  # 返回字符的 ASCII 数值，或者 Unicode 数值\n",
    "        if cp == 0 or cp == 0xfffd or _is_control(char):  # cp 为 0 表示无意义的字符\n",
    "            continue\n",
    "        if _is_whitespace(char):\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            output.append(char)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0xfffd 显示为 �，通常用于替换未知的字符。_is_control 用于判断一个字符是否是控制字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_control(char):\n",
    "    \"\"\"检查字符是否是控制字符\"\"\"\n",
    "    # 这里把回车换行和 tab 认为是 whitespace\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return false\n",
    "    cat = unicodedata.category(char)  # 这个函数返回 unicode 字符的 Category\n",
    "    if cat in (\"Cc\", \"Cf\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是调用 _is_whitespace 函数，把 whitespace 变成空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(char):\n",
    "    \"\"\"检查字符是否是空格\"\"\"\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里把 category 为 Zs 的字符以及空格、tab、回车和换行当成 whitespace。然后是 _tokenize_chinese_chars，用于切分中文，这里的切分中文是把中文切分成一个一个的汉字。也就是在中文字符的前后加上空格，这样后续的分词流程会把每一个字符当成一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_chinese_chars(self, text):\n",
    "    \"\"\"汉字周围加上空格\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cp = ord(char)\n",
    "        if self._is_chinese_char(cp):\n",
    "            output.append(\" \")\n",
    "            output.append(char)\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            output.append(char)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的关键是调用 _is_chinese_char 函数，用于判断一个 unicode 字符是否是中文字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_chinese_char(self, cp):\n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是使用 whitespace 进行分词，这里通过 whitespace_tokenize 来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后遍历每一个词，如果 do_lower_case=True，先用 lower() 函数变成小写，接着调用 _run_strip_accents 函数去除 accents："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_strip_accents(self, text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat == \"Mn\":\n",
    "            continue\n",
    "        output.append(char)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理完大小写和 accent 之后得到的 Token 通过函数 _run_split_on_punc 再次用标点切分。这个函数会对输入字符串用标点进行切分，返回一个 list，list 的每个元素都是一个 char，比如输入 he's，输出是 [[h,e],['],[s]]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def _run_split_on_punc(self, text):\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        if _is_punctuation(char):\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        i += 1\n",
    "    \n",
    "    return [\"\".join(x) for x in output]\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "        (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', \"'\", 's']\n",
      "['窗 前 明 月 光', '，', '疑 是 地 上 霜 ', '。']\n"
     ]
    }
   ],
   "source": [
    "print(_run_split_on_punc(None, \"he's\"))\n",
    "print(_run_split_on_punc(None, \"窗 前 明 月 光，疑 是 地 上 霜 。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordpieceTokenizer\n",
    "\n",
    "WordpieceTokenizer 的作用是把词再切分成更细粒度的 WordPiece。对于中文来说，WordpieceTokenizer 什么也不干，因为之前的分词已经是基于字符的了。一般情况下不需要自己重新生成 WordPiece，使用 BERT 模型里自带的就行。\n",
    "\n",
    "WordpieceTokenizer 的代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(self, text):\n",
    "    # 把一段文字切分成 word piece。这其实是贪心的最大正向匹配算法。\n",
    "    # 比如：input = \"unaffable\", output = [\"un\", \"##aff\", \"##able\"]\n",
    "    text = convert_to_unicode(text)\n",
    "    \n",
    "    output_tokens = []\n",
    "    for token in whitespace_tokenize(text):\n",
    "        chars = list(token)\n",
    "        if len(chars) > self.max_input_chars_per_word:  # 200\n",
    "            output_tokens.append(self.unk_token)\n",
    "            continue\n",
    "            \n",
    "        is_bad = False\n",
    "        start = 0\n",
    "        sub_tokens = []\n",
    "        while start < len(chars):\n",
    "            end = len(chars)\n",
    "            cur_substr = None\n",
    "            while start < end:\n",
    "                substr = \"\".join(chars[start:end])\n",
    "                if start > 0:\n",
    "                    substr = \"##\" + substr\n",
    "                if substr in self.vocab:\n",
    "                    cur_substr = substr\n",
    "                    break\n",
    "                end -= 1\n",
    "            if cur_substr is None:\n",
    "                is_bad = True\n",
    "                break\n",
    "            sub_tokens.append(cur_substr)\n",
    "            start = end\n",
    "            \n",
    "        if is_bad:\n",
    "            output_tokens.append(self.unk_token)\n",
    "        else:\n",
    "            output_tokens.extend(sub_tokens)\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_classifier.py 的 main 函数\n",
    "\n",
    "main 函数的主要代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n",
    "    \n",
    "    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)  # 从 json file 中构建 BertConfig\n",
    "    \n",
    "    task_name = FLAGS.task_name.lower()  # 任务名\n",
    "    processor = processors[task_name]()  # 根据任务名获取 processor\n",
    "    label_list = processor.get_labels()\n",
    "    \n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "    \n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        master=FLAGS.master,\n",
    "        model_dir=FLAGS.output_dir,\n",
    "        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "            iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "            num_shards=FLAGS.num_tpu_cores,\n",
    "            per_host_input_for_training=is_per_host))\n",
    "    \n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    num_warmup_steps = None\n",
    "    if FLAGS.do_train:\n",
    "        train_examples = processor.get_train_examples(FLAGS.data_dir)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / FLAGS.train_batch_size * FALGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "        \n",
    "    model_fn = model_fn_builder(\n",
    "        bert_config=bert_config,\n",
    "        num_labels=len(label_list),\n",
    "        init_checkpoint=FLAGS.init_checkpoint,\n",
    "        learning_rate=FLAGS.learning_rate,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        use_tpu=FLAGS.use_tpu,\n",
    "        use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "    \n",
    "    # If TPU is not available, this will fall back to normal Estimator on CPU or GPU\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=FLAGS.use_tpu,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        train_batch_size=FLAGS.train_batch_size,\n",
    "        eval_batch_size=FLAGS.eval_batch_size,\n",
    "        predict_batch_size=FLAGS.predict_batch_size)\n",
    "    \n",
    "    if FLAGS.do_train:\n",
    "        train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
    "        file_based_convert_examples_to_features(\n",
    "            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
    "        \n",
    "        train_input_fn = file_based_input_fn_builder(\n",
    "            input_file=train_file,\n",
    "            seq_length=FLAGS.max_seq_length,\n",
    "            is_training=True,\n",
    "            drop_remainder=True)\n",
    "        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "        \n",
    "    if FLAGS.do_eval:\n",
    "        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n",
    "        eval_file = os.path.join(FLAGS.output_dir, \"eval.tf_record\")\n",
    "        file_based_convert_examples_to_features(\n",
    "            eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n",
    "        \n",
    "        # This tells the estimator to run through the entire set.\n",
    "        eval_steps = None\n",
    "        \n",
    "        eval_drop_remainder = True if FLAGS.use_tpu else False\n",
    "        eval_input_fn = file_based_input_fn_builder(\n",
    "            input_file=eval_file,\n",
    "            seq_length=FLAGS.max_seq_length,\n",
    "            is_training=False,\n",
    "            drop_remainder=eval_drop_remainder)\n",
    "        \n",
    "        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "        \n",
    "    if FLAGS.do_predict:\n",
    "        predict_examples = processor.get_test_examples(FALGS.data_dir)\n",
    "        predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
    "        file_based_convert_examples_to_features(predict_examples, label_list,\n",
    "                                               FLAGS.max_seq_length, tokenizer, predict_file)\n",
    "        \n",
    "        predict_drop_remainder = True if FLAGS.use_tpu else False\n",
    "        predict_input_fn = file_based_input_fn_builder(\n",
    "            input_file=predict_file,\n",
    "            seq_length=FLAGS.max_seq_length,\n",
    "            is_training=False,\n",
    "            drop_remainder=predict_drop_remainder)\n",
    "        \n",
    "        result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用的是 Tensorflow 的 Estimator API。训练、验证和预测的代码都很相似，这里只介绍训练部分的代码。\n",
    "\n",
    "首先是通过 file_based_convert_examples_to_features 函数把输入的 tsv 文件转换为 TFRecord 文件，便于 Tensorflow 处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_convert_examples_to_features(\n",
    "    examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"Convert a set of 'InputExample's to a TFRecord file.\"\"\"\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    \n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        feature = convert_single_example(ex_index, example, label_list,\n",
    "                                        max_seq_length, tokenizer)\n",
    "        \n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "        \n",
    "        features = collections.OrderDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
    "        \n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_based_convert_examples_to_features 函数遍历每一个 example(InputExample 类的对象)。然后使用 convert_single_example 函数把每个 InputExample 对象变成 InputFeature。InputFeature 是一个存放特征的对象，它包括 input_ids, input_mask, segment_ids 和 label_id，这 4 个属性除了 label_id 是一个 int 之外，其它都是 int 的列表。因此调用 create_int_feature 函数时，label_id 需要构造一个只有一个元素的列表。最后构造 tf.train.Example 对象，然后写到 TFRecord 文件里。\n",
    "\n",
    "这里的关键是 convert_single_example 函数，读懂了它就真正明白 BERT 把输入表示成向量的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Converts a single 'InputExample' into a single 'InputFeatures'.\"\"\"\n",
    "    \n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "        \n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        \n",
    "    if tokens_b:\n",
    "        # 如果有 b，那么需要保留 3 个特殊 token: [CLS], [SEP], [SEP]\n",
    "        # 如果两个序列加起来太长，需要裁剪\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # 没有 b，只需要保留 [CLS], [SEP] 两个特殊字符\n",
    "        # 如果 token 太多，直接截取后面的部分\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0: max_seq_length - 2]\n",
    "            \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "        \n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "        \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # The mask has 1 for real tokens and 0 for padding tokens.\n",
    "    # Only real tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        \n",
    "    label_id = label_map[example.label]\n",
    "    \n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        is_real_example=True)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果两个 token 序列的长度太长，那么需要去掉一些，这会用到 _truncate_seq_pair 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    \n",
    "    # 每次从较长的 sequence 中去掉一个 token\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数比较简单，如果两个序列的长度小于 max_length，那么不用 truncate，否则在 tokens_a 和 tokens_b 中选择较长的那个裁掉最后那个 token，直到两个序列的长度小于等于 max_length。\n",
    "\n",
    "对于 Estimator API 来说，最重要的是实现 model_fn 和 input_fn。我们先看 input_fn，它是由 file_based_input_fn_builder 构造出来的。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                               drop_remainder):\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    def _decode_record(record, name_to_features):\n",
    "        # 把 record decode 成 Tensorflow example\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "        \n",
    "        # tf.Example 只支持 tf.int64，TPU 只支持 tf.int32，所以把所有 int64 转为 int32\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "            \n",
    "        return example\n",
    "    \n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        \n",
    "        # 对于训练来说，会重复的并行读取和 shuffling\n",
    "        # 对于验证，不需要 shuffling 和并行读取\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "        \n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size=batch_size,\n",
    "                drop_remainder=drop_remainder))\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数返回一个函数 input_fn。这个 input_fn 函数首先从文件得到 TFRecordDataset，然后根据是否训练来 shuffle 和重复读取。然后用 apply 函数对每一个 TFRecord 进行 map_and_batch，调用 _decode_record 函数对 record 进行 parsing。从而把 TFRecord 的一条 Record 变成 tf.Example 对象，这个对象包括了 input_ids 等 4 个用于训练的 Tensor。\n",
    "\n",
    "接下来是 model_fn_builder，它用于构造 Estimator 使用的 model_fn，下面是它的主要代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "    \"\"\"Returns 'model_fn' closure for TPUEstimator\"\"\"\n",
    "    \n",
    "    # features 表示输入（特征），labels 表示输出\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "        \n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        \n",
    "        # 创建 Transformer 模型，这是最主要的代码\n",
    "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, use_one_hot_embeddings)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        # 从 checkpoint 恢复参数\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            \n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "            \n",
    "        output_spec = None\n",
    "        # 构造训练的 spec\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = optimization.create_optimizer(\n",
    "                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "            \n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op = train_op,\n",
    "                scaffold_fn=scaffold_fn)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            def metric_fn(per_example_loss, label_idds, logits):\n",
    "                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predictions)\n",
    "                loss = tf.metrics.mean(per_example_loss)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"eval_loss\": loss,\n",
    "                }\n",
    "            \n",
    "            eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metrics=eval_metrics,\n",
    "                scaffold_fn=scaffold_fn)\n",
    "        else:\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=probabilities,\n",
    "                scaffold_fn=scaffold_fn)\n",
    "        return output_spec\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的代码都是一些模板代码，没什么可说的。最重要的是调用 create_model 创建 Transformer 模型。下面我们来看这个函数的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "    \"\"\"创建分类模型\"\"\"\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "    \n",
    "    # 在这里，我们是用来做分类，因此只需得到 [CLS] 最后一层的输出\n",
    "    # 如果需要做序列标注，就需要使用 model.get_sequence_output()\n",
    "    # 默认参数下，它返回的 output_layer 是 [8, 768]\n",
    "    output_layer = model.get_pooled_output()\n",
    "    \n",
    "    # 默认是 768\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    \n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            # 0.1 的 dropout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "            \n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        \n",
    "        return (loss, per_example_loss, logits, probabilities)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码调用 modeling.BertModel 得到 Bert 模型，然后调用它的 get_pooled_output 方法得到 [CLS] 最后一层的输出，这是一个 768（默认参数下）的向量，然后就是接一个全连接层得到 logits，softmax 计算得到概率，之后就可以根据真实的分类标签计算 loss。关键的代码就是 modeling.BertModel。\n",
    "\n",
    "### BertModel 类\n",
    "\n",
    "这个类是最终定义模型的地方。我们首先来看这个类的用法，把它当成黑盒，下面的代码演示了 BertModel 的使用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设输入已经分词并且变成 Wordpiece 的 id 了，输入是 [2, 3]，表示 batch_size=2, max_seq_length=3\n",
    "input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "# 第一个例子的实际长度是 3，第二个例子的实际长度是 2\n",
    "input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
    "# 第一个例子的 3 个 token 中前两个属于句子 1，第三个属于句子 2；第二个例子的第一个 token 属于句子 1，第二个属于句子 2，第三个是 padding\n",
    "token_type_ids = tf.constant([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "# 创建一个 BertConfig，词典大小是 32000，Transformer 的隐单元个数是 512，\n",
    "# 8 个 Transformer block，每个 block 有 8 个 Attention Head，全连接层的隐单元是 1024\n",
    "config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "                             num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "# 创建 BertModel\n",
    "model = modeling.BertModel(config=config, is_training=True,\n",
    "                           input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "label_embeddings = tf.get_variable(...)\n",
    "pooled_output = model.get_pooled_output()\n",
    "logits = tf.matmul(pooled_output, label_embeddings)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们看一下 BertModel 的构造函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "    \"\"\"BERT model\"\"\"\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                 input_ids,\n",
    "                 input_mask=None,\n",
    "                 token_type_ids=None,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 scope=None):\n",
    "        \"\"\"Constructor for BertModel.\n",
    "        \n",
    "        Args:\n",
    "          config: 'BertConfig' instance.\n",
    "          is_training: bool 值，表示是训练还是 eval，控制是否应用 dropout\n",
    "          input_ids: int32 的 Tensor，shape 为 [batch_size, seq_length]\n",
    "          input_mask: 可选，int32 的 Tensor，shape 为 [batch_size, seq_length]\n",
    "          token_type_ids: 可选，int32 的 Tensor，shape 为 [batch_size, seq_length]\n",
    "          use_one_hot_embeddings: 可选，bool 值。如果是 True，使用矩阵乘法实现提取词的 embedding，否则使用 tf.embedding_lookup()；\n",
    "                                  对于 TPU，前者更快，对于 GPU，CPU，后者更快\n",
    "          scope: 可选，变量的 scope，默认是 bert\n",
    "          \n",
    "        Raises:\n",
    "          ValueError: The config is invalid or one of the input tensor shapes is invalid\n",
    "        \"\"\"\n",
    "        config = copy.deepcopy(config)\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0\n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "            \n",
    "        input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_length = input_shape[1]\n",
    "        \n",
    "        if input_mask is None:\n",
    "            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)  # 设为全 1 矩阵\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)  # 设为全 0，表示都属于一个句子\n",
    "            \n",
    "        with tf.variable_scope(scope, default_name=\"bert\"):\n",
    "            with tf.variable_scope(\"embeddings\"):\n",
    "                # Perform embedding lookup on the word ids.\n",
    "                (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
    "                    input_ids=input_ids,\n",
    "                    vocab_size=config.vocab_size,\n",
    "                    embedding_size=config.hidden_size,\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    word_embedding_name=\"word_embeddings\",\n",
    "                    use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "                \n",
    "                # 增加位置 embeddings 和 token type 的 embeddings，然后是 layer normalize 和 dropout。\n",
    "                self.embedding_output = embedding_postprocessor(\n",
    "                    input_tensor=self.embedding_output,\n",
    "                    use_token_type=True,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    token_type_vocab_size=config.type_vocab_size,\n",
    "                    token_type_embedding_name=\"token_type_embeddings\",\n",
    "                    use_position_embeddings=True,\n",
    "                    position_embedding_name=\"position_embeddings\",\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    max_position_embeddings=config.max_position_embeddings,\n",
    "                    dropout_prob=config.hidden_dropout_prob)\n",
    "                \n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                # 把 shape 为 [batch_size, seq_length] 的 2D mask 变成 shape 为 [batch_size, seq_length, seq_length] 的 3D mask\n",
    "                # 用作 attention scores\n",
    "                attention_mask = create_attention_mask_from_input_mask(\n",
    "                    input_ids, input_mask)\n",
    "                \n",
    "                # Run the stacked transformer\n",
    "                # 'sequence_output' shape = [batch_size, seq_length, hidden_size]\n",
    "                self.all_encoder_layers = transformer_model(\n",
    "                    input_tensor=self.embedding_output,\n",
    "                    attention_mask=attention_mask,\n",
    "                    hidden_size=config.hidden_size,\n",
    "                    num_hidden_layers=config.num_hidden_layers,\n",
    "                    num_attention_heads=config.num_attention_heads,\n",
    "                    intermediate_size=config.intermediate_size,\n",
    "                    intermediate_act_fn=get_activation(config.hidden_act),\n",
    "                    hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    do_return_all_layers=True)\n",
    "                \n",
    "            # sequence_output 是最后一层的输出，shape 是 [batch_size, seq_length, hidden_size]\n",
    "            self.sequence_output = self.all_encoder_layers[-1]\n",
    "            \n",
    "            # \"pooler\" 把 encoded sequence tensfor [batch_size, seq_length, hidden_size] 转化为\n",
    "            # [batch_size, hidden_size]，这对 segment-level 的分类任务是必需的，因为我们需要固定维度表示的 segment\n",
    "            with tf.variable_scope(\"pooler\"):\n",
    "                # 取第一个 token 对应的 hidden state\n",
    "                # sequence_output[:, 0:1, :] 得到的是 [batch_size, 1, hidden_size]\n",
    "                # 使用 squeeze 把第二维去掉\n",
    "                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "                self.pooled_output = tf.layers.dense(\n",
    "                    first_token_tensor,\n",
    "                    config.hidden_size,\n",
    "                    activation=tf.tanh,\n",
    "                    kernel_initializer=create_initializer(config.initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码很长，其实很简单。首先是对 config 深度拷贝一份，如果不是训练，把 dropout 都置为零。如果输入的 input_mask 为 None，就构造一个 shape 合适值全为 1 的 input_mask，这表示输入都是有效的；如果 token_type_ids 为 None，就构造一个 shape 合适并且值全为 0 的 tensor，表示所有 token 都属于第一个句子。\n",
    "\n",
    "然后使用 embedding_lookup 函数构造词的 embedding，用 embedding_postprocessor 函数增加位置 embeddings 和 token type 的 embeddings，然后是 layer normalize 和 dropout。\n",
    "\n",
    "接着用 transformer_model 函数构造多个 Transformer Sublayer 然后 stack 在一起。得到的 all_encoder_layers 是一个 list，长度是 num_hidden_layers（默认是 12），每一层对应一个值。每个值都是一个 shape 为 [batch_size, seq_length, hidden_size] 的 tensor。\n",
    "\n",
    "self.sequence_output 是最后一层的输出，shape 是 [batch_size, seq_length, hidden_size]。first_token_tensor 是第一个 token ([CLS]) 最后一层的输出，shape 是 [batch_size, hidden_size]。最后对 self.sequence_output 再加一个线性变换，得到的 tensor 仍然是 [batch_size, hidden_size]。\n",
    "\n",
    "embedding_lookup 函数用于实现 Embedding，它有两种方式：使用 tf.nn.embedding_lookup 和矩阵乘法（one_hot_embedding=True）。前者适合于 CPU 和 GPU，后者适合于 TPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(input_ids,\n",
    "                     vocab_size,\n",
    "                     embedding_size=128,\n",
    "                     initializer_range=0.02,\n",
    "                     word_embedding_name=\"word_embeddings\",\n",
    "                     use_one_hot_embeddings=False):\n",
    "    \"\"\"Looks up words embeddings for id tensor.\n",
    "    \n",
    "    Args:\n",
    "      input_ids: int32 Tensor shape 为 [batch_size, seq_length]，表示 Wordpiece 的 id\n",
    "      vocab_size: 词典大小\n",
    "      embedding_size: word embeddings 的宽度\n",
    "      initializer_range: float, 随机初始化的范围\n",
    "      word_embedding_name: string, embedding 表的名称\n",
    "      use_one_hot_embeddings: bool, 如果是 true，使用 one-hot 方法实现 embedding，否则使用 'tf.gather(tf.nn.embedding_lookup)'的方式\n",
    "      \n",
    "    Returns:\n",
    "      float tensor, shape 为 [batch_size, seq_length, embedding_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 这个函数假设输入的 shape 是 [batch_size, seq_length, num_inputs]\n",
    "    # 普通的 embedding 一般假设输入是 [batch_size, seq_length]\n",
    "    # 增加 num_inputs 是为了一次计算更多的 embedding，目前的代码没有用到，传入的 input_ids 都是 2 维的\n",
    "    \n",
    "    # 如果输入是 [batch_size, seq_length]，把它 reshape 成 [batch_size, seq_length, 1]\n",
    "    if input_ids.shape.ndims == 2:\n",
    "        input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
    "        \n",
    "    embedding_table = tf.get_variable(\n",
    "        name=word_embedding_name,\n",
    "        shape=[vocab_size, embedding_size],\n",
    "        initializer=create_initializer(initializer_range))\n",
    "    \n",
    "    flat_input_ids = tf.reshape(input_ids, [-1])  # 把 input_ids 铺平成 1-D 数组 [batch_size * seq_length]\n",
    "    if use_one_hot_embeddings:\n",
    "        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)  # [batch_size * seq_length, vocab_size]\n",
    "        output = tf.matmul(one_hot_input_ids, embedding_table)  # [batch_size * seq_length, embedding_size]\n",
    "    else:\n",
    "        output = tf.gather(embedding_table, flat_input_ids)  # [batch_size * seq_length, embedding_size]\n",
    "        \n",
    "    input_shape = get_shape_list(input_ids)\n",
    "    \n",
    "    # 把 output 变成 [batch_size, seq_length, num_inputs * embedding_size]\n",
    "    output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
    "    \n",
    "    return (output, embedding_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以不用关心太多的细节，把这个函数当成黑盒，那么只需要知道如果输入 input_ids 为 [8, 128]，输出是 [8, 128, 768] 就可以了。\n",
    "\n",
    "函数 embedding_postprocessor 的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_postprocessor(input_tensor,\n",
    "                            use_token_type=False,\n",
    "                            token_type_ids=None,\n",
    "                            token_type_vocab_size=16,\n",
    "                            token_type_embedding_name=\"token_type_embeddings\",\n",
    "                            use_position_embeddings=True,\n",
    "                            position_embedding_name=\"position_embeddings\",\n",
    "                            initializer_range=0.02,\n",
    "                            max_position_embeddings=512,\n",
    "                            dropout_prob=0.1):\n",
    "    \"\"\"对 word embedding 之后的 tensor 进行后处理\n",
    "    Args:\n",
    "      input_tensor: float tensor，shape 为 [batch_size, seq_length, embedding_size]\n",
    "      use_token_type: bool. 是否增加 token_type_ids 的 embeddings\n",
    "      token_type_ids:（可选）int32 tensor，shape 为 [batch_size, seq_length]，如果 use_token_type 为 true，则必须提供值\n",
    "      token_type_vocab_size: int, token type 的个数，通常是 2\n",
    "      token_type_embedding_name: string, Token type embedding 的名字\n",
    "      use_position_embedding: bool, 是否使用位置 embedding\n",
    "      position_embedding_name: string, 位置 embedding 的名字\n",
    "      initializer_range: float，初始化范围\n",
    "      max_position_embeddings: int，位置编码的最大长度，可以比最大序列长度大，但不能比它小\n",
    "      dropout_prob: float, Dropout 概率\n",
    "      \n",
    "    Returns:\n",
    "      float tensor shape 和 input_tensor 相同\n",
    "    \"\"\"\n",
    "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    width = input_shape[2]\n",
    "    \n",
    "    output = input_tensor\n",
    "    \n",
    "    if use_token_type:\n",
    "        if token_type_ids is None:\n",
    "            raise ValueError(\"'token_type_ids' must be specified if 'use_token_type' is True.\")\n",
    "        token_type_table = tf.get_variable(\n",
    "            name=token_type_embedding_name,\n",
    "            shape=[token_type_vocab_size, width],\n",
    "            initializer=create_initializer(initializer_range))\n",
    "        # 因为 token type 通常很小，所以直接用矩阵乘法（one-hot）更快\n",
    "        flat_token_type_ids = tf.reshape(token_type_ids, [-1])  # [batch_size * seq_length]\n",
    "        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)  # [batch_size * seq_length, token_type_vocab_size]\n",
    "        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)  # [batch_size * seq_length, width]\n",
    "        token_type_embeddings = tf.reshape(token_type_embeddings,\n",
    "                                           [batch_size, seq_length, width])\n",
    "        output += token_type_embeddings\n",
    "        \n",
    "    if use_position_embeddings:\n",
    "        assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)  # 确保 seq_length <= max_position_embeddings\n",
    "        with tf.control_dependencies([assert_op]):\n",
    "            full_position_embeddings = tf.get_variable(\n",
    "                name=position_embedding_name,\n",
    "                shape=[max_position_embeddings, width],\n",
    "                initializer=create_initializer(initializer_range))\n",
    "            # position embedding 是学习的参数，我们用一个长序列长度 max_position_embeddings 来创建它。\n",
    "            # 真实的序列长度可能比这个值小，为了提高训练速度，通过 tf.slice 取出 [0, 1, 2, ..., seq_length - 1] 的部分\n",
    "            position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
    "                                           [seq_length, -1])\n",
    "            num_dims = len(output.shape.as_list())  # 3\n",
    "            \n",
    "            # word embedding 之后的 tensor 是 [batch_size, seq_length, width]\n",
    "            # 因为位置编码是于输入内容无关的，它的 shape 总是 [seq_length, width]\n",
    "            # 我们无法把位置 embedding 加到 word embedding 上\n",
    "            # 因此我们需要扩展位置编码为 [1, seq_length, width]\n",
    "            # 然后通过 broadcasting 加上去\n",
    "            position_broadcast_shape = []\n",
    "            for _ in range(num_dims - 2):\n",
    "                position_broadcast_shape.append(1)\n",
    "            position_broadcast_shape.extent([seq_length, width])\n",
    "            position_embeddings = tf.reshape(position_embeddings,\n",
    "                                             position_broadcast_shape)\n",
    "            output += position_embeddings\n",
    "            \n",
    "    output = layer_norm_and_dropout(output, dropout_prob)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_attention_mask_from_input_mask 函数用于构造 Mask 矩阵。我们先了解以下它的作用然后再阅读其代码。比如调用它时的两个参数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [[1, 2, 3, 0, 0],\n",
    "             [1, 3, 5, 6, 1]]\n",
    "input_mask = [[1, 1, 1, 0, 0],\n",
    "              [1, 1, 1, 1, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表示这个 batch 有两个样本，第一个样本长度为 3，第二个样本长度为 5。在计算 self-attention 的时候每个样本都需要一个 Attention Mask 矩阵，表示每一个时刻可以 attend to 的范围，1 表示可以 attend，0 表示是 padding 的（或者在机器翻译的 decoder 中不能 attend to 未来的词）。对于上面的输入，这个函数返回一个 shape 为 [2, 5, 5] 的 tensor，分别代表两个 Attention Mask 矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [1, 1, 1, 0, 0],  # 它表示第 1 个词可以 attend to 3 个词\n",
    "    [1, 1, 1, 0, 0],  # 它表示第 2 个词可以 attend to 3 个词\n",
    "    [1, 1, 1, 0, 0],  # 它表示第 3 个词可以 attend to 3 个词\n",
    "    [1, 1, 1, 0, 0],  # 无意义，因为输入第 4 个词是 padding\n",
    "    [1, 1, 1, 0, 0]   # 无意义，因为输入第 5 个词是 padding\n",
    "]\n",
    "[\n",
    "    [1, 1, 1, 1, 1],  # 它表示第 1 个词可以 attend to 5 个词\n",
    "    [1, 1, 1, 1, 1],  # 它表示第 2 个词可以 attend to 5 个词\n",
    "    [1, 1, 1, 1, 1],  # 它表示第 3 个词可以 attend to 5 个词\n",
    "    [1, 1, 1, 1, 1],  # 它表示第 4 个词可以 attend to 5 个词\n",
    "    [1, 1, 1, 1, 1],  # 它表示第 5 个词可以 attend to 5 个词\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
    "    \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
    "    \n",
    "    Args:\n",
    "      form_tensor: 2D or 3D tensor，shape 为 [batch_size, from_seq_length, ...]\n",
    "      to_mask: int32 tensor, shape 为 [batch_size, to_seq_length]\n",
    "      \n",
    "    Returns:\n",
    "      float tensor, shape 为 [batch_size, from_seq_length, to_seq_length]\n",
    "    \"\"\"\n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "    \n",
    "    to_shape = get_shape_list(to_mask, expected_rank=2)\n",
    "    to_seq_length = to_shape[1]\n",
    "    \n",
    "    to_mask = tf.cast(\n",
    "        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
    "    \n",
    "    broadcast_ones = tf.ones(\n",
    "        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
    "    \n",
    "    mask = broadcast_ones * to_mask\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是 transformer_model 函数，它是构造 Transformer 的核心代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit\"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "t = np.arange(-4.0, 4.0, 0.01)\n",
    "s = gelu(t)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, s.eval())\n",
    "ax.grid()\n",
    "plt.show()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(input_tensor,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False):\n",
    "    \"\"\"Multi-headed, multi-layer Transformer from 'Attention is all you need'.\n",
    "    \n",
    "    See the original paper:\n",
    "    https://arxiv.org/abs/1706.03762\n",
    "\n",
    "    Also see:\n",
    "    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
    "    \n",
    "    Args:\n",
    "      input_tensor: float Tensor, shape 为 [batch_size, seq_length, hidden_size]\n",
    "      attention_mask: 可选，int32 Tensor, shape 为 [batch_size, seq_length, seq_length]，1 表示可以 attend to，0 不能\n",
    "      hidden_size: int. Transformer 隐单元个数\n",
    "      num_hidden_layers: int. 有多少个 layers (blocks)\n",
    "      num_attention_heads: int. Transformer 中 attention heads 的个数\n",
    "      intermediate_size: int. 全连接层的隐单元个数\n",
    "      intermediate_act_fn: function. 全连接层的激活函数\n",
    "      hidden_dropout_prob: float. 隐层的 dropout 概率\n",
    "      attention_probs_dropout_prob: float. attention 的 dropout 概率\n",
    "      initializer_range: float. 初始化范围\n",
    "      do_return_all_layers: 返回所有层的输出还是最后一层的输出\n",
    "      \n",
    "    Returns:\n",
    "      float tensor, shape 为 [batch_size, seq_length, hidden_size]\n",
    "    \"\"\"\n",
    "    if hidden_size % num_attention_heads != 0:\n",
    "        raise ValueError(\n",
    "            \"The hidden size (%d) is not a multiple of the number of attention\"\n",
    "            \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        \n",
    "    attention_head_size = int(hidden_size / num_attention_heads)\n",
    "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    input_width = input_shape[2]\n",
    "    \n",
    "    # Transformer 在所有层上进行残差连接，输入需要和 hidden_size 大小一致\n",
    "    if input_width != hidden_size:\n",
    "        raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" % (input_width, hidden_size))\n",
    "        \n",
    "    prev_output = reshape_to_matrix(input_tensor)\n",
    "    \n",
    "    all_layer_outputs = []\n",
    "    for layer_index in range(num_hidden_layers):\n",
    "        with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
    "            layer_input = prev_output\n",
    "            \n",
    "            with tf.variable_scope(\"attention\"):\n",
    "                attention_heads = []\n",
    "                with tf.variable_scope(\"self\"):\n",
    "                    attention_head = attention_layer(\n",
    "                        from_tensor=layer_input,\n",
    "                        to_tensor=layer_input,\n",
    "                        attention_mask=attention_mask,\n",
    "                        num_attention_heads=num_attention_heads,\n",
    "                        size_per_head=attention_head_size,\n",
    "                        attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                        initializer_range=initializer_range,\n",
    "                        do_return_2d_tensor=True,\n",
    "                        batch_size=batch_size,\n",
    "                        from_seq_length=seq_length,\n",
    "                        to_seq_length=seq_length)\n",
    "                    attention_heads.append(attentio_head)\n",
    "                    \n",
    "                attention_output = None\n",
    "                if len(attention_heads) == 1:\n",
    "                    attention_output = attention_heads[0]\n",
    "                else:\n",
    "                    # 如果有多个 head，需要把多个 head 的输出拼接起来\n",
    "                    attention_output = tf.concat(attention_heads, axis=-1)\n",
    "                    \n",
    "                # Run a linear projection of 'hidden_size' then add a residual with 'layer_input'\n",
    "                with tf.variable_scope(\"output\"):\n",
    "                    attention_output = tf.layers.dense(\n",
    "                        attention_output,\n",
    "                        hidden_size,\n",
    "                        kernel_initializer=create_initializer(initializer_range))\n",
    "                    attention_output = dropout(attention_output, hidden_dropout_prob)\n",
    "                    attention_output = layer_norm(attention_output + layer_input)\n",
    "                    \n",
    "            # 这里就是 Transformer 里的 position-wise feed-forward network，包括两层全连接层\n",
    "            # 只有这层有激活函数\n",
    "            with tf.variable_scope(\"intermediate\"):\n",
    "                intermediate_output = tf.layers.dense(\n",
    "                    attention_output,\n",
    "                    intermediate_size,\n",
    "                    activation=intermediate_act_fn,\n",
    "                    kernel_initializer=create_initializer(initializer_range))\n",
    "                \n",
    "            # Down-project back to 'hidden_size' then add the residual\n",
    "            with tf.variable_scope(\"output\"):\n",
    "                layer_output = tf.layers.dense(\n",
    "                    intermediate_output,\n",
    "                    hidden_size,\n",
    "                    kernel_initializer=create_initializer(initializer_range))\n",
    "                layer_output = dropout(layer_output, hidden_dropout_prob)\n",
    "                layer_output = layer_norm(layer_output + attention_output)\n",
    "                prev_output = layer_output\n",
    "                all_layer_outputs.append(layer_output)\n",
    "                \n",
    "    if do_return_all_layers:\n",
    "        final_outputs = []\n",
    "        for layer_output in all_layer_outputs:\n",
    "            final_output = reshape_from_matrix(layer_output, input_shape)\n",
    "            final_outputs.append(final_output)\n",
    "        return final_outputs\n",
    "    else:\n",
    "        final_output = reshape_from_matrix(prev_output, input_shape)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对照 Transformer 的论文，非常容易阅读，里面实现 Self-Attention 的函数就是 attention_layer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(from_tensor,\n",
    "                    to_tensor,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    size_per_head=512,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    do_return_2d_tensor=False,\n",
    "                    batch_size=None,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None):\n",
    "    \"\"\"Performs multi-headed attention from 'from_tensor' to 'to_tensor'.\n",
    "    \n",
    "    这是 Transformer 论文中 multi-headed 注意力的实现，如果 'from_tensor' 和 'to_tensor' 相同，那么就是 self-attention。\n",
    "    每个时刻，'from_tensor' 都会 attends to 'to_tensor'，返回固定长度 vector。\n",
    "    \n",
    "    该函数首先把 'from_tensor' 变换为 \"query\" tensor，把 'to_tensor' 变换为 \"key\" \"value\" tensors，\n",
    "    总共有 'num_attention_heads' 组 query, key, value，每个 tensor 的 shape 都是 [batch_size, seq_length, size_per_head]\n",
    "    \n",
    "    具体的实现和 Transformer 论文上说的一致。在实现时，multi-headed attention 是通过 transpose 和 reshape 实现的，而不是真的分割 tensors。\n",
    "    \n",
    "    Args:\n",
    "      from_tensor: float Tensor, shape 为 [batch_size, from_seq_length, from_width].\n",
    "      to_tensor: float Tensor，shape 为 [batch_size, to_seq_length, to_width].\n",
    "      attention_mask:（可选）int32 Tensor，shape 为 [batch_size, from_seq_length, to_seq_length]，值可以是 0 或 1。\n",
    "        在计算 attention score 的时候，会把 0 变成负无穷（绝对值很大的负数），而 1 不变，这样 softmax 的时候，前者会趋近于 0，从而实现 mask 的功能更\n",
    "      num_attention_heads: int, Attention heads 的数量\n",
    "      size_per_head: int. 每个 attention head 的大小\n",
    "      query_act: (可选)，query 变换的激活函数\n",
    "      key_act: (可选)，key 变换的激活函数\n",
    "      value_act: （可选），value 变换的激活函数\n",
    "      attention_probs_dropout_prob: （可选），attention 的 dropout 概率\n",
    "      initializer_range: float, 初始化范围\n",
    "      do_return_2d_tensor: bool, 如果是 True，返回的 shape 为 [batch_size * from_seq_length, num_attention_heads * size_per_head]\n",
    "        如果是 False，返回 [batch_size, from_seq_length, num_attention_heads * size_per_head]\n",
    "      batch_size: (可选)，如果输入是 3D 的，那么 batch 就是第一维，但是可能 3D 的压缩成了 2D，所以需要告诉函数 batch_size\n",
    "      from_seq_length：（可选），同上，需要告诉函数 from_seq_length\n",
    "      to_seq_length：（可选），同上。\n",
    "      \n",
    "    Returns:\n",
    "      根据 do_return_2d_tensor 的值，返回不同维度的 tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
    "                             seq_length, width):\n",
    "        output_tensor = tf.reshape(\n",
    "            input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
    "        \n",
    "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "        return output_tensor\n",
    "    \n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
    "    \n",
    "    if len(from_shape) != len(to_shape):\n",
    "        raise ValueError(\n",
    "            \"The rank of 'from_tensor' must match the rank of 'to_tensor'.\")\n",
    "        \n",
    "    if len(from_shape) == 3:\n",
    "        batch_size = from_shape[0]\n",
    "        from_seq_length = from_shape[1]\n",
    "        to_seq_length = to_shape[1]\n",
    "    elif len(from_shape) == 2:  #如果压缩成 2D 的，一定要传入这三个参数，否则抛异常\n",
    "        if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
    "            raise ValueError(\n",
    "                \"When passing in rank 2 tensors to attention_layer, the values \"\n",
    "                \"for 'batch_size', 'from_seq_length', and 'to_seq_length' \"\n",
    "                \"must all be specified.\")\n",
    "            \n",
    "    # B = batch_size (number of sequences)\n",
    "    # F = 'from_tensor' sequence length\n",
    "    # T = 'to_tensor' sequence length\n",
    "    # N = 'num_attention_heads'\n",
    "    # H = 'size of head'\n",
    "    \n",
    "    from_tensor_2d = reshape_to_matrix(from_tensor)\n",
    "    to_tensor_2d = reshape_to_matrix(to_tensor)\n",
    "    \n",
    "    # 'query_layer' = [B * F, N * H]\n",
    "    query_layer = tf.layers.dense(\n",
    "        from_tensor_2d,\n",
    "        num_attention_heads * size_per_head,\n",
    "        activation=query_act,\n",
    "        name=\"query\",\n",
    "        kernel_initializer=create_initializer(initializer_range))\n",
    "    \n",
    "    # 'key_layer' = [B * T, N * H]\n",
    "    key_layer = tf.layers.dense(\n",
    "        to_tensor_2d,\n",
    "        num_attention_heads * size_per_head,\n",
    "        activation=key_act,\n",
    "        name=\"key\",\n",
    "        kernel_initializer=create_initializer(initializer_range))\n",
    "    \n",
    "    # 'value_layer' = [B * T, N * H]\n",
    "    value_layer = tf.layers.dense(\n",
    "        to_tensor_2d,\n",
    "        num_attention_heads * size_per_head,\n",
    "        activation=value_act,\n",
    "        name=\"value\",\n",
    "        kernel_initializer=create_initializer(initializer_range))\n",
    "    \n",
    "    # 'query_layer' = [B, N, F, H]\n",
    "    query_layer = transpose_for_scores(query_layer, batch_size,\n",
    "                                       num_attention_heads, from_seq_length,\n",
    "                                       size_per_head)\n",
    "    \n",
    "    # 'key_layer' = [B, N, T, H]\n",
    "    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
    "                                     to_seq_length, size_per_head)\n",
    "    \n",
    "    # 计算 query 和 key 的内积，得到 attention scores\n",
    "    # attention_scores = [B, N, F, T]\n",
    "    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "    accention_scores = tf.multiply(attention_scores,\n",
    "                                   1.0 / math.sqrt(float(size_per_head)))\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        # 'attention_mask' = [B, 1, F, T]\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "        \n",
    "        # 如果 mask 是 1，那么 (1-1) * -10000=0，adder=0，如果 mask 是 0，那么 (1-0) * -10000=-10000，adder=-10000\n",
    "        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "        \n",
    "        # 把 adder 加到 attention_scores，如果 mask 为 1 相当于加 0，mask 为 0 相当于加 -10000\n",
    "        # 通常 attention_scores 不会很大，因此 mask 为 0 相当于把 attention_scores 设为负无穷，softmax 的时候值趋近于0\n",
    "        attention_scores += adder\n",
    "        \n",
    "    # 'attention_probs' = [B, N, F, T]\n",
    "    attention_probs = tf.nn.softmax(attention_scores)\n",
    "    \n",
    "    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
    "\n",
    "    # `value_layer` = [B, T, N, H]\n",
    "    value_layer = tf.reshape(\n",
    "        value_layer,\n",
    "        [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
    "\n",
    "    # `value_layer` = [B, N, T, H]\n",
    "    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
    "\n",
    "    # `context_layer` = [B, N, F, H]\n",
    "    context_layer = tf.matmul(attention_probs, value_layer)\n",
    "\n",
    "    # `context_layer` = [B, F, N, H]\n",
    "    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
    "\n",
    "    if do_return_2d_tensor:\n",
    "        # `context_layer` = [B*F, N*H]\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer,\n",
    "            [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
    "    else:\n",
    "        # `context_layer` = [B, F, N*H]\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer,\n",
    "            [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
    "\n",
    "    return context_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自己进行 Pretraining\n",
    "\n",
    "虽然 Google 提供了 pretraining 的模型，我们也可以自己通过 mask LM 和 Next Sentence Prediction 进行 Pretraining。可以从头开始训练，也可以用 Google 提供的 checkpoint 作为初始值。\n",
    "\n",
    "要进行 Pretraining 首选要有数据，数据由很多“文档”组成，每篇文档的句子之间是有关系的。如果只能拿到没有关系的句子则是无法训练的，训练数据需要变成如下的格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This text is included to make sure Unicode is handled properly: 力加勝北区ᴵᴺᵀᵃছজটডণত\n",
    "Text should be one-sentence-per-line, with empty lines between documents.\n",
    "This sample text is public domain and was randomly selected from Project Guttenberg.\n",
    "\n",
    "The rain had only ceased with the gray streaks of morning at Blazing Star, and the settlement awoke to a moral sense of cleanliness, and the finding of forgotten knives, tin cups, and smaller camp utensils, where the heavy showers had washed away the debris and dust heaps before the cabin doors.\n",
    "Indeed, it was recorded in Blazing Star that a fortunate early riser had once picked up on the highway a solid chunk of gold quartz which the rain had freed from its incumbering soil, and washed into immediate and glittering popularity.\n",
    "Possibly this may have been the reason why early risers in that locality, during the rainy season, adopted a thoughtful habit of body, and seldom lifted their eyes to the rifted or india-ink washed skies above them.\n",
    "\"Cass\" Beard had risen early that morning, but not with a view to discovery.\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据是文本文件，每一行表示一个句子，空行表示一个文档的结束（新文档的开始），比如上面的例子，共有 2 篇文档，第一个文档有 3 个句子，第二个文档有多个句子。\n",
    "\n",
    "我们首先需要使用 create_pretraining_data.py 把文本文件变成 TFRecord 格式，便于后面的代码进行 pretraing。这个脚本会把整个文本加载到内存中，因此这个文件不能太大。如果有很多文档需要训练，比如 1000万，那么可以把这 1000万文档拆分成 1万个文件，每个文件 1000 个文档，从而生成 1000 个 TFRecord 文件。\n",
    "\n",
    "我们先看 create_pretraining_data.py 的用法：\n",
    "```\n",
    "python create_pretraining_data.py \\\n",
    "    --input_file=./sample_text.txt \\\n",
    "    --output_file=/tmp/tf_examples.tfrecord \\\n",
    "    --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n",
    "    --do_lower_case=True \\\n",
    "    --max_seq_length=128 \\\n",
    "    --max_predictions_per_seq=20 \\\n",
    "    --masked_lm_prob=0.15 \\\n",
    "    --random_seed=12345 \\\n",
    "    --dupe_factor=5\n",
    "```\n",
    "\n",
    "* max_seq_length: Token 序列的最大长度\n",
    "* max_predictions_per_seq: 最多生成多少个 mask\n",
    "* masked_lm_prob: 多少比例的 token 变成 mask\n",
    "* dupe_factor: 一个文档重复多少次\n",
    "\n",
    "首先说一下 dupe_factor，比如一个句子 \"it is a good day\"，为了充分利用数据，我们可以多次随机生成 mask，比如第一次可能生成 \"it is a [MASK] day\"，第二次可能生成 \"it [MASK] a good day\"。这个参数控制重复的次数。\n",
    "\n",
    "masked_lm_prob 就是论文里的参数 15%，max_predictions_per_seq 是一个序列最多 mask 多少个 token，它通常等于 max_seq_length * masked_lm_prob。\n",
    "\n",
    "我们先看 main 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "    \n",
    "    input_files = []\n",
    "    # input_file 可以传多个文件名，以 ',' 分隔\n",
    "    for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "        input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "        \n",
    "    rng = random.Random(FLAGS.random_seed)\n",
    "    instances = create_training_instances(\n",
    "        input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "        FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "        rng)\n",
    "\n",
    "    output_files = FLAGS.output_file.split(\",\")\n",
    "\n",
    "    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                    FLAGS.max_predictions_per_seq, output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main 函数很简单，输入文本文件列表是 input_files，通过函数 create_training_instance 构建训练的 instances，然后调用 write_instance_to_example_files 以 TFRecord 格式写到 output_files。\n",
    "\n",
    "我们先来看一个训练样本的格式，这是用类 TrainingInstance 来表示的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "                 is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_positions = masked_lm_positions\n",
    "        self.masked_lm_labels = masked_lm_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设原始的两个句子为: \"it is a good day\" 和 \"I want to go out\"，那么处理后的 TrainingInstance 可能为：\n",
    "```\n",
    "1. tokens = [\"[CLS]\", \"it\", \"is\", \"a\", \"[MASK]\", \"day\", \"[SEP]\", \"I\", \"apple\", \"to\", \"go\", \"out\", \"[SEP]\"]\n",
    "2. segment_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "3. is_random_next=False\n",
    "4. masked_lm_position=[4, 8, 9]\n",
    "  表示 mask 后为 [\"[CLS]\", \"it\", \"is\", \"a\", \"[MASK]\", \"day\", \"[SEP]\", \"I\", \"[MASK]\", \"to\", \"go\", \"out\", \"[SEP]\"]\n",
    "5. masked_lm_labels=[\"good\", \"want\", \"to\"]\n",
    "```\n",
    "is_random_next 表示这两句话是有关联的，预测句子关系的分类器应该把这个输入判断为 1。masked_lm_positions 记录哪些位置被 mask 了，而 masked_lm_labels 记录被 mask 之前的词。\n",
    "\n",
    "注意：tokens 已经处理过了，good 被替换成 [MASK]，而 want 被替换成 apple，to 替换为它自己。根据 masked_lm_positions、masked_lm_labels 和 tokens 是可以恢复出原始（分词后）句子的。\n",
    "\n",
    "create_training_instances 函数的代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "    \"\"\"Create 'TrainingInstance' from raw text.\"\"\"\n",
    "    all_documents = [[]]\n",
    "    \n",
    "    # 输入文件格式：\n",
    "    # （1）每行一个句子。这应该是实际的句子，不应该是整个段落或者段落的随机片段，\n",
    "    #  因为我们需要使用句子边界来做下一个句子的预测。\n",
    "    # （2）文档之间有一个空行。我们会认为同一个文档的相邻句子是有关系的\n",
    "    for input_file in input_files:\n",
    "        with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "            while True:\n",
    "                line = tokenization.convert_to_unidoce(reader.readline())\n",
    "                if not line:\n",
    "                    break\n",
    "                line = line.strip()\n",
    "                \n",
    "                # 空行表示文档分隔符\n",
    "                if not line:\n",
    "                    all_documents.append([])\n",
    "                tokens = tokenizer.tokenize(line)\n",
    "                if tokens:\n",
    "                    all_documents[-1].append(tokens)\n",
    "                    \n",
    "    # 删除空文档\n",
    "    all_documents = [x for x in all_documents if x]\n",
    "    rng.shuffle(all_documents)\n",
    "    \n",
    "    vocab_words = list(tokenizer.vocab.keys())\n",
    "    instances = []\n",
    "    for _ in range(dupe_factor):\n",
    "        # 遍历所有文档\n",
    "        for document_index in range(len(all_documents)):\n",
    "            # 从一个文档里抽取多个 TrainingInstance\n",
    "            instances.extend(\n",
    "                create_instances_from_document(\n",
    "                    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "                    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "    \n",
    "    rng.shuffle(instances)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码会调用 create_instances_from_document 来从一个文档里抽取多个训练数据。普通的语言模型只要求连续的字符串就行，通常是把所有的文本（比如维基百科）拼接成一个很大很大的文本文件，然后训练的时候随机从里面抽取固定长度的字符串作为一个“句子”。但是 BERT 要求输入是一个一个的 document，每个 document 有很多句子，这些句子是连贯的真实句子，需要正确的分句。而不能随机切分句子，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates 'TrainingInstance's for a single document.\"\"\"\n",
    "    document = all_documents[document_index]\n",
    "    \n",
    "    # 为 [CLS], [SEP], [SEP] 留 3 个位置\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "    \n",
    "    # 我们通常想要填充整个序列，因为不管怎样都要 padding 至 'max_seq_length'，这样短的序列会浪费资源，\n",
    "    # 但有时候我们会希望生成一些短的句子，因为在实际中会有短句，如果都是长句子，那么就容易出现 mismatch，\n",
    "    # 所以我们以 short_seq_prob=0.1 的概率生成短句子。\n",
    "    target_seq_length = max_num_tokens\n",
    "    if rng.random() < short_seq_prob:\n",
    "        target_seq_length = rng.randint(2, max_num_tokens)\n",
    "        \n",
    "    # 不能把文档中的所有 token 拼接成一个长句子然后随机分割，这样使得下个句子预测太过容易，\n",
    "    # 而是在真实句子中分割成 A 和 B。\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # 'a_end' 是第一个句子 A（current_chunk 里）结束的下标\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "                    \n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                    \n",
    "                tokens_b = []\n",
    "                # Random next\n",
    "                is_random_text = False\n",
    "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "                    \n",
    "                    # 随机挑选另外一篇文档的随机开始的句子，为确保和当前文档不一样，这里循环 10 次\n",
    "                    for _ in range(10):\n",
    "                        random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "                        if random_document_index != document_index:\n",
    "                            break\n",
    "                    \n",
    "                    # 随机挑选文档\n",
    "                    random_document = all_documents[random_document_index]\n",
    "                    # 随机选择开始句子\n",
    "                    random_start = rng.randint(0, len(random_document) - 1)\n",
    "                    # 把 token 加到 tokens_b 里，一直到大于 target_b_length 为止\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "                    # 之前虽然挑选了 len(current_chunk) 个句子，但是 a_end 之后的句子替换成随机的其它文档的句子\n",
    "                    # 因此并没有使用 a_end 之后的句子，因此我们修改下标 i，使得下一次循环可以再次使用这些句子\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                # 真实的句子\n",
    "                else:\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                        \n",
    "                # 如果长度比 max_num_tokens 大，需要裁剪一下\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "                \n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "                \n",
    "                # 处理句子 A\n",
    "                tokens.append(\"[CLS]\")\n",
    "                segment_ids.append(0)\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                # 处理句子 B\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "                (tokens, masked_lm_positions, masked_lm_labels) = create_masked_lm_predictions(\n",
    "                    tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "                instance = TrainingInstance(\n",
    "                    tokens=tokens,\n",
    "                    segment_ids=segment_ids,\n",
    "                    is_random_next=is_random_next,\n",
    "                    masked_lm_positions=masked_lm_positions,\n",
    "                    masked_lm_labels=masked_lm_labels)\n",
    "                instances.append(instance)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码有点长，但是逻辑很简单，比如有一篇文档有n个句子：\n",
    "```\n",
    "w11,w12,.....,\n",
    "w21,w22,....\n",
    "wn1,wn2,....\n",
    "```\n",
    "那么算法首先找到一个 chunk，它会不断往 chunk 加入一个句子的所有 Token，使得 chunk 里的 token 数量大于等于 target_seq_length。通常我们期望target_seq_length 为 max_num_tokens(128-3)，这样 padding 的尽量少，训练的效率高。但是有时候我们也需要生成一些短的序列，否则会出现训练与实际使用不匹配的问题。\n",
    "\n",
    "找到一个 chunk 之后，比如这个 chunk 有 5 个句子，那么我们随机的选择一个切分点，比如 3。把前 3 个句子当成句子 A，后两个句子当成句子 B。这是两个句子 A 和 B 有关系的样本(is_random_next=False)。为了生成无关系的样本，我们还以 50% 的概率把 B 用随机从其它文档抽取的句子替换掉，这样就得到无关系的样本(is_random_next=True)。如果是这种情况，后面两个句子需要放回去，以便在下一层循环中能够被再次利用。\n",
    "\n",
    "有了句子 A 和 B 之后，我们就可以填充 tokens 和 segment_ids，这里会加入特殊的 [CLS] 和 [SEP]。接下来使用 create_masked_lm_predictions 来随机的选择某些 Token，把它变成 [MASK]。其代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "    \n",
    "    # [CLS] 和 [SEP] 不能用于 mask\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        # whole word masking 表示我们需要 mask 对应原始单词的所有 wordpieces。\n",
    "        # 当一个单词被分割为 wordpieces 时，第一个 token 是没有标记的，后续的 token 有前缀 ##。\n",
    "        # 所以，每次看到 ## 时，就把它 append 到上一个 word index 里。\n",
    "        # 注意，whole word masking 不会改变训练代码，我们仍独立地预测每个 wordpiece，并在整个词汇表上 softmax\n",
    "        if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and token.startswith(\"##\")):\n",
    "            cand_indexes[-1].append(i)\n",
    "        else:\n",
    "            cand_indexes.append([i])\n",
    "            \n",
    "    rng.shuffle(cand_indexes)\n",
    "    \n",
    "    output_tokens = list(tokens)\n",
    "    \n",
    "    # 需要被模型预测的 token 个数 min(max_predictions_per_seq(20), 实际 token 数 * 15%)\n",
    "    num_to_predict = min(max_predictions_per_seq,\n",
    "                         max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    \n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for index_set in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        # 如果增加 whole-word mask 后超过了 predictions 的最大数目，则直接跳过\n",
    "        if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "            continue\n",
    "        is_any_index_covered = False\n",
    "        for index in index_set:\n",
    "            if index in covered_indexes:\n",
    "                is_any_index_covered = True\n",
    "                break\n",
    "        if is_any_index_covered:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            covered_indexes.add(index)\n",
    "            \n",
    "            masked_token = None\n",
    "            # 80% 的概率替换成 [MASK]\n",
    "            if rng.random() < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                # 10% 的概率保持不变\n",
    "                if rng.random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                # 10% 的概率随机替换一个 word\n",
    "                else:\n",
    "                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "                    \n",
    "            output_tokens[index] = masked_token\n",
    "            \n",
    "            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "    \n",
    "    # 按照下标排序，保证是句子中出现的顺序。\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "    \n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "        \n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后是使用函数 write_instance_to_example_files 把前面得到的 TrainingInstance 写到文件里，这个函数的代码是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "    \"\"\"Create TF example files from 'TrainingInstance's.\"\"\"\n",
    "    writers = []\n",
    "    for output_file in output_files:\n",
    "        writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "    \n",
    "    writer_index = 0\n",
    "    \n",
    "    total_written = 0\n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        \n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "        \n",
    "        masked_lm_positions = list(instance.masked_lm_positions)\n",
    "        masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "        masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "        \n",
    "        while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "            masked_lm_positions.append(0)\n",
    "            masked_lm_ids.append(0)\n",
    "            masked_lm_weights.append(0.0)\n",
    "            \n",
    "        next_sentence_label = 1 if instance.is_random_next else 0\n",
    "        \n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "        features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "        features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "        features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "        features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "        \n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        \n",
    "        writers[writer_index].write(tf_example.SerializeToString())\n",
    "        writer_index = (writer_index + 1) % len(writers)\n",
    "        \n",
    "        total_written += 1\n",
    "        \n",
    "    for writer in writers:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用run_pretraining.py脚本进行Pretraining。用法为：\n",
    "```\n",
    "python run_pretraining.py \\\n",
    "\t--input_file=/tmp/tf_examples.tfrecord \\\n",
    "\t--output_dir=/tmp/pretraining_output \\\n",
    "\t--do_train=True \\\n",
    "\t--do_eval=True \\\n",
    "\t--bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n",
    "\t--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n",
    "\t--train_batch_size=32 \\\n",
    "\t--max_seq_length=128 \\\n",
    "\t--max_predictions_per_seq=20 \\\n",
    "\t--num_train_steps=20 \\\n",
    "\t--num_warmup_steps=10 \\\n",
    "\t--learning_rate=2e-5\n",
    "```\n",
    "参数都比较容易理解，通常我们需要调整的是num_train_steps、num_warmup_steps和learning_rate。run_pretraining.py的代码和run_classifier.py很类似，都是用BertModel构建Transformer模型，唯一的区别在于损失函数不同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "    \n",
    "    (masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "        bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "        masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "    \n",
    "    (next_sentence_loss, next_sentence_example_loss, next_sentence_log_probs) = get_next_sentence_output(\n",
    "        bert_config, model.get_pooled_output(), next_sentence_labels)\n",
    "    \n",
    "    total_loss = masked_lm_loss + next_sentence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_masked_lm_output 函数用于计算语言模型的 Loss (Mask 位置预测的词和真实的词是否相同)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,\n",
    "                         label_ids, label_weights):\n",
    "    \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "    \n",
    "    # 只需要 mask 位置的 token 输出\n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "    \n",
    "    with tf.variable_scope(\"cls/predictions\"):\n",
    "        # 在输出之前再加一个非线性变换，这些参数只是用于训练，在 fine-tuning 的时候就不用了\n",
    "        with tf.variable_scope(\"transform\"):\n",
    "            input_tensor = tf.layers.dense(input_tensor,\n",
    "                                           units=bert_config.hidden_size,\n",
    "                                           activation=modeling.get_activation(bert_config.hidden_act),\n",
    "                                           kernel_initializer=modeling.create_initializer(\n",
    "                                               bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "            \n",
    "        # output_weights 是复用输入的 word embedding，所以是传入的，这里多加一个 bias\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\",\n",
    "            shape=[bert_config.vocab_size],\n",
    "            initializer=tf.zeros_initializer())\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        label_ids = tf.reshpe(label_ids, [-1])\n",
    "        label_weights = tf.reshape(label_weights, [-1])\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(\n",
    "            label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
    "        \n",
    "        # 'positions' tensor 可能是 zero-padded 的。\n",
    "        # 'label_weights' tensor 里面的 1 表示真实的预测，0 表示 padding 预测\n",
    "        per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "        numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "        denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "        loss = numerator / denominator\n",
    "        \n",
    "    return (loss, per_example_loss, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_next_sentence_output 函数用于计算预测下一个句子的 loss，代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sentence_output(bert_config, input_tensor, labels):\n",
    "    \"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n",
    "    \n",
    "    # 简单的二分类，0 表示下一个句子，1 表示随机的句子。weight matrix 在 pre-training 后不被使用\n",
    "    with tf.variable_scope(\"cls/seq_relationship\"):\n",
    "        output_weights = tf.get_variable(\n",
    "            \"output_weights\",\n",
    "            shape=[2, bert_config.hidden_size],\n",
    "            initializer=modeling.create_initializer(bert_config.initializer_range))\n",
    "        outpub_bias = tf.get_variable(\n",
    "            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
    "        \n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        labels = tf.reshape(labels, [-1])\n",
    "        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, per_example_loss, log_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
