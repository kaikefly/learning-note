{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking and padding with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding sequence data\n",
    "\n",
    "处理序列数据的时候，每个样本一般都有不同的长度，如下：\n",
    "\n",
    "[\n",
    "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
    "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
    "  [\"Hello\", \"world\", \"!\"]\n",
    "]\n",
    "\n",
    "经过词表 lookup 后，数据可能变成这样：\n",
    "\n",
    "[\n",
    "  [83, 91, 1, 645, 1253, 927],\n",
    "  [73, 8, 3215, 55, 927],\n",
    "  [71, 1331, 4231]\n",
    "]\n",
    "\n",
    "这个 2D list 具有不同的长度，由于深度学习模型的输入数据必须为一个一样的 tensor (比如 shape 为 (batch_size, 6, vocab_size))，所以短句子需要填充，和长句子保持一样(或者把长句子进行裁剪)。\n",
    "\n",
    "Keras 提供了一个 API，可以方便地进行裁剪和填充：tf.keras.preprocessing.sequence.pad_sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  83   91    1  645 1253  927]\n",
      " [  73    8 3215   55  927    0]\n",
      " [ 711  632   71    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [711, 632, 71]\n",
    "]\n",
    "\n",
    "# 一般是使用 0 作为填充标志\n",
    "# 可以使用前填充（在头部）和后填充（在尾部），在 RNN 层中推荐使用后填充（为了可以使用 CuDNN 实现）\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                              padding='post')\n",
    "print(padded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking\n",
    "\n",
    "现在所有的样本都有了相同的长度，但必须要告诉模型有些部分实际上是填充的，计算时应该被忽略，这就需要 masking。有三种方式：\n",
    "1. 增加一个 keras.layers.Masking 层；\n",
    "2. 配置 keras.layers.Embedding 层为 mask_zero=True;\n",
    "3. 调用层时手动传一个 mask 参数。\n",
    "\n",
    "#### Mask-generating layers: Embedding and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True False False False]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "print(masked_output._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True False False False]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "masking_layer = layers.Masking()\n",
    "unmasked_embedding = tf.cast(\n",
    "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]),\n",
    "    tf.float32)\n",
    "masked_embedding = masking_layer(unmasked_embedding)\n",
    "print(masked_embedding._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask propagation in the Functional API and Sequential API\n",
    "\n",
    "当使用 Functional 或 Sequential API 时，Embedding 或 Masking 层产生的 mask 可以在网络任意层中传播。Keras 可以自动获取 input 对应的 mask。但在 subclassed 的模型或层中的 call 方法中，mask 不会自动传播，必须手动传 mask 参数。在下面的 Sequential 模型中，LSTM 层可以自动取到 mask，并忽略掉填充的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True),\n",
    "    layers.LSTM(32)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的 Functional API 模型也是如此："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "outputs = layers.LSTM(32)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing mask tensors directly to layers\n",
    "\n",
    "可以处理 masks（比如 LSTM）的层在 __call__ 方法中都有 mask 参数，同时，产生 mask 的层（比如 Embedding）都有 compute_mask(input, previous_mask) 可以调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4750, shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-5.4335361e-04,  4.5744404e-03, -2.5518630e-03, ...,\n",
       "        -1.2719275e-03,  4.5639314e-04,  1.7747076e-02],\n",
       "       [ 4.0839505e-03,  2.5836329e-04, -1.4106500e-03, ...,\n",
       "         6.3203569e-03, -3.2956379e-03,  6.1929859e-03],\n",
       "       [-3.7946794e-03, -4.6657557e-03, -3.6887231e-03, ...,\n",
       "         2.7973948e-03, -1.7516615e-03,  3.3951143e-03],\n",
       "       ...,\n",
       "       [ 4.0407674e-03,  1.0803455e-03, -5.1380898e-04, ...,\n",
       "         3.5285761e-04, -5.5053877e-03,  4.0466321e-04],\n",
       "       [ 1.7662996e-03,  5.3380225e-03,  4.0492108e-03, ...,\n",
       "         1.0195142e-03, -3.0410581e-03, -1.4063568e-03],\n",
       "       [ 8.2265725e-03,  9.9298632e-06, -7.7274878e-04, ...,\n",
       "        -3.0296063e-03, -1.0024327e-02,  3.2026954e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "        self.lstm = layers.LSTM(32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        mask = self.embedding.compute_mask(inputs)\n",
    "        output = self.lstm(x, mask=mask)\n",
    "        return output\n",
    "    \n",
    "layer = MyLayer()\n",
    "x = np.random.random((32, 10)) * 100\n",
    "x = x.astype('int32')\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting masking in your custom layers\n",
    "\n",
    "有时候需要写生成 mask 的层（如 Embedding），或者需要修改当前 mask 的层。为了实现这个功能，层需要实现 layer.compute_mask() 方法，其根据输入和当前 mask 生成新的 mask。例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True False False False]], shape=(3, 6), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]], shape=(3, 3), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[ True  True  True]\n",
      " [ True  True False]\n",
      " [False False False]], shape=(3, 3), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class TemporalSplit(tf.keras.layers.Layer):\n",
    "    \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n",
    "        # subtensors along the time axis (axis 1).\n",
    "        return tf.split(inputs, 2, axis=1)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Also split the mask into 2 if it presents.\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return tf.split(mask, 2, axis=1)\n",
    "    \n",
    "first_half, second_half = TemporalSplit()(masked_embedding)\n",
    "print(masked_embedding._keras_mask)\n",
    "print(first_half._keras_mask)\n",
    "print(second_half._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=4757, shape=(2, 1, 3), dtype=int32, numpy=\n",
       " array([[[1, 2, 3]],\n",
       " \n",
       "        [[7, 8, 9]]])>,\n",
       " <tf.Tensor: id=4758, shape=(2, 1, 3), dtype=int32, numpy=\n",
       " array([[[ 4,  5,  6]],\n",
       " \n",
       "        [[10, 11, 12]]])>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[[1, 2, 3],\n",
    "                  [4, 5, 6]],\n",
    "                 [[7, 8, 9],\n",
    "                  [10, 11, 12]]]) # (2, 2, 3)\n",
    "\n",
    "b = tf.split(a, 2, axis=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[False  True  True  True  True  True False  True  True  True]\n",
      " [ True False  True  True  True  True  True False  True  True]\n",
      " [ True  True  True  True  True False False  True  True  True]], shape=(3, 10), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class CustomEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
    "        super(CustomEmbedding, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mask_zero = mask_zero\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer='random_normal',\n",
    "            dtype='float32')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        return tf.not_equal(inputs, 0)\n",
    "    \n",
    "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
    "x = np.random.random((3, 10)) * 9\n",
    "x = x.astype('int32')\n",
    "\n",
    "y = layer(x)\n",
    "mask = layer.compute_mask(x)\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing layers that need mask information\n",
    "\n",
    "有些层需要根据 mask 做出判断，在 call 中接受 mask 参数，来判断是否跳过某些时间步。可以在 call 中加入 mask=None 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConsumer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
