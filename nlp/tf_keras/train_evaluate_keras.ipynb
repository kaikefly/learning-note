{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种情况：\n",
    "1. 使用 built-in APIs 来训练和验证（如 model.fit(), model.evaluate(), model.predict()）\n",
    "2. 使用 eager execution 和 GradientTape 写 custom loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Using built-in training & evaluation loops\n",
    "\n",
    "当向内置的 training loops 传数据时，应该使用 Numpy arrays (如果数据量小，内存满足) 或 tf.data.Dataset。在下面的例子中，使用 MNIST datasets，Numpy array 形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面就是典型的 end-to-end workflow，包括训练、验证、测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              # List of metrics to monitor\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 2s 42us/sample - loss: 0.3552 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.2206 - val_sparse_categorical_accuracy: 0.9365\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 28us/sample - loss: 0.1720 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.1452 - val_sparse_categorical_accuracy: 0.9580\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 28us/sample - loss: 0.1230 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.1209 - val_sparse_categorical_accuracy: 0.9633\n",
      "\n",
      "history dict: {'loss': [0.3551591438007355, 0.17198494044065477, 0.12303630582809448], 'sparse_categorical_accuracy': [0.8999, 0.94826, 0.96318], 'val_loss': [0.22055140339136123, 0.14517438811659814, 0.12091187521070242], 'val_sparse_categorical_accuracy': [0.9365, 0.958, 0.9633]}\n"
     ]
    }
   ],
   "source": [
    "print('# Fit model on training data')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=3,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "10/1 [============================================================================================================================================================================================================================================================================================================] - 0s 300us/sample - loss: 0.1107 - sparse_categorical_accuracy: 0.9000\n",
      "test loss, test acc:  [0.11069478839635849, 0.9]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using 'evaluate'\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test[:10], y_test[:10], batch_size=128)\n",
    "print('test loss, test acc: ', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(x_test[:3])\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specifying a loss, metrics, and an optimizer**\n",
    "\n",
    "使用 fit 训练模型时，需要指定 loss function, optimizer 和 metrics to monitor (可选)，传入 compile() 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics 是个数组，可以指定多个 metrics。如果模型有多个 outputs，可以对每个 output 指定不同的 losses 和 metrics，而且可以指定权重。也可以使用字符串表示 optimizer, losses, metrics："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了后面使用方便，将模型定义和编译阶段分开，后面将会多次调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='digits')\n",
    "    x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = layers.Dense(10, name='predictions')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**有很多 built-in 的 optimizer, losses, metrics 可以使用**\n",
    "\n",
    "Optimizers:\n",
    "* SGD() (with or without momentum)\n",
    "* RMSprop()\n",
    "* Adam()\n",
    "* etc.\n",
    "\n",
    "Losses:\n",
    "* MeanSquaredError()\n",
    "* KLDivergence()\n",
    "* CosineSimilarity()\n",
    "* etc.\n",
    "\n",
    "Metrics:\n",
    "* AUC()\n",
    "* Precision()\n",
    "* Recall()\n",
    "* etc.\n",
    "\n",
    "**Custom losses**\n",
    "\n",
    "也可以自己定制 losses，下面两个例子展示了两种方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 1s 28us/sample - loss: 1.4328\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 22us/sample - loss: 0.7512\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 22us/sample - loss: 0.6090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead2d6bf60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_loss_function(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=basic_loss_function)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果除了 y_true 和 y_pred 还有别的参数，可以继承 tf.keras.losses.Loss，然后实现 __init__(self) 和 call(self, y_true, y_pred) 函数，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBinaryCrossEntropy(keras.losses.Loss):\n",
    "    def __init__(self, pos_weight, weight, from_logits=False,\n",
    "                 reduction=keras.losses.Reduction.AUTO,\n",
    "                 name='weighted_binary_crossentropy'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.from_logits = from_logits\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        ce = tf.losses.binary_crossentropy(\n",
    "            y_true, y_pred, from_logits=self.from_logits)[:,None]\n",
    "        ce = self.weight * (ce*(1-y_true) + self.pos_weight*ce*(y_true))\n",
    "        return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 2s 32us/sample - loss: 0.1618\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 1s 25us/sample - loss: 0.0629\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0477\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 1s 25us/sample - loss: 0.0389\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 1s 27us/sample - loss: 0.0328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead2bbd5f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y_train = tf.one_hot(y_train.astype(np.int32), depth=10)  # 需要将 y_train 转为 one-hot 格式\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=WeightedBinaryCrossEntropy(\n",
    "        pos_weight=0.5, weight=2, from_logits=True)\n",
    ")\n",
    "model.fit(x_train, one_hot_y_train, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom metrics**\n",
    "\n",
    "也可以实现自己的 Metrics，通过继承 tf.metrics.Metrics，并实现四个方法：\n",
    "* __init__(self), in which you will create state variables for your metric.\n",
    "* update_state(self, y_true, y_pred, sample_weight=None), which uses the targets y_true and the model predictions y_pred to update the state variables.\n",
    "* result(self), which uses the state variables to compute the final results.\n",
    "* reset_states(self), which reinitializes the state of the metric.\n",
    "\n",
    "下面的例子展示了怎么实现 CategoricalTruePositives metric, 计算多少样例被正确分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTruePositives(keras.metrics.Metric):\n",
    "    def __init__(self, name='categorical_true_positives', **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, 'int32') == tf.cast(y_pred, 'int32')\n",
    "        values = tf.cast(values, 'float32')\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, 'float32')\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "    \n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 2s 32us/sample - loss: 0.0694 - categorical_true_positives: 48955.0000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0589 - categorical_true_positives: 49100.0000\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 25us/sample - loss: 0.0505 - categorical_true_positives: 49229.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead2c08d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[CategoricalTruePositives()])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling losses and metrics that don't fit the standard signature**\n",
    "\n",
    "绝大多数的 losses 和 metrics 都可以通过 y_ture 和 y_pred 求出，但有的，比如 regularization loss 只需要层的信息。这种情况下，需要在层的 call 函数中调用 self.add_loss(loss_value)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 32us/sample - loss: 2.5089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead6c4d160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "        return inputs  # Pass-through layer.\n",
    "    \n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对 metrics 可以同样这么做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 31us/sample - loss: 0.3350 - std_of_activation: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead7251ba8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # The `aggregation` argument defines\n",
    "        # how to aggregate the per-batch values\n",
    "        # over each epoch:\n",
    "        # in this case we simply average them.\n",
    "        self.add_metric(keras.backend.std(inputs),\n",
    "                        name='std_of_activation',\n",
    "                        aggregation='mean')\n",
    "        return inputs  # Pass-through layer.\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Functional API 中，也可以调用 model.add_loss(loss_tensor) 或 model.add_metric(metric_tensor, name, aggregation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 35us/sample - loss: 2.5533 - std_of_activation: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead88bec88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x1 = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x2 = layers.Dense(64, activation='relu', name='dense_2')(x1)\n",
    "outputs = layers.Dense(10, name='predictions')(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1),\n",
    "                 name='std_of_activation',\n",
    "                 aggregation='mean')\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatically setting apart a validation holdout set**\n",
    "\n",
    "除了显式指定 validation_data 外，还可以使用 validation_split 自动地从训练数据中留一部分用于验证，validation_split 的值表示比例，比如 0.2 表示训练数据中 20% 的数据用于验证。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "   64/40000 [..............................] - ETA: 5:43 - loss: 2.3050 - sparse_categorical_accuracy: 0.1406 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead8f96940>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training & evaluation from tf.data Datasets**\n",
    "\n",
    "tf.data API 用于导入和预处理数据，比较快速和可拓展，可以直接将 Dataset 实例传到 fit(), evaluate() 和 predict() 中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3359 - sparse_categorical_accuracy: 0.9051\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1595 - sparse_categorical_accuracy: 0.9528\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1155 - sparse_categorical_accuracy: 0.9665\n",
      "\n",
      "# Evaluate\n",
      "1250/1250 [==============================] - 1s 722us/step - loss: 0.1421 - sparse_categorical_accuracy: 0.9586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.14213139407945272, 'sparse_categorical_accuracy': 0.9586}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(8)\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# You can also evaluate\n",
    "print('\\n# Evaluate')\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，Dataset 在每个 epoch 结束时会重置，所以在下一个 epoch 可以复用。\n",
    "\n",
    "如果只想使用 Dataset 一部分 batches 用于训练，可以使用 steps_per_epoch 参数。这种情况下，在每个 epoch 结束时，dataset 不会重置，而是接着用剩下的 batches："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Train for 100 steps\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.8357 - sparse_categorical_accuracy: 0.7856\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3846 - sparse_categorical_accuracy: 0.8909\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3132 - sparse_categorical_accuracy: 0.9091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eaddc5bda0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "print(len(x_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2014).batch(64).repeat()\n",
    "\n",
    "# Only use the 100 batches per epoch (that's 64 * 100 samples)\n",
    "model.fit(train_dataset, steps_per_epoch=100, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以将 Dataset 实例作为 validation_data 参数传入 fit："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3421 - sparse_categorical_accuracy: 0.9029 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1621 - sparse_categorical_accuracy: 0.9521 - val_loss: 0.1329 - val_sparse_categorical_accuracy: 0.9593\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1173 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.1187 - val_sparse_categorical_accuracy: 0.9638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ead6c19ac8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每轮结束时，模型都会根据 validation Dataset 计算出 validation loss 和 metrics。如果想对一部分 batches 执行 validation，可以传入 validation_steps："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3342 - sparse_categorical_accuracy: 0.9045 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1561 - sparse_categorical_accuracy: 0.9526 - val_loss: 0.1973 - val_sparse_categorical_accuracy: 0.9422\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1135 - sparse_categorical_accuracy: 0.9648 - val_loss: 0.1683 - val_sparse_categorical_accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eada9a0710>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3,\n",
    "          # Only run validation using the first 10 batches of the dataset\n",
    "          # using the `validation_steps` argument\n",
    "          validation_data=val_dataset, validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当用 Dataset 对象训练模型时，validation_split 不可用。\n",
    "\n",
    "**Other input formats supported**\n",
    "\n",
    "除了 Numpy arrays 和 Tensorflow Datasets 外，还可以使用 Pandas dataframes 或 Python generators that yield batches 来训练。但通常，如果数据量比较小，推荐使用 Numpy，否则使用 Datasets。\n",
    "\n",
    "**Using sample weighting and class weighting**\n",
    "\n",
    "除了输入数据和目标数据外，还可以传 sample weights, class weights 到 fit 中：\n",
    "* When training from Numpy data: via the sample_weight and class_weight arguments.\n",
    "* When training from Datasets: by having the Dataset return a tuple (input_batch, target_batch, sample_weight_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit with class weight\n",
      "Train on 50000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 1s 29us/sample - loss: 0.1004 - sparse_categorical_accuracy: 0.9717\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0833 - sparse_categorical_accuracy: 0.9764\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0716 - sparse_categorical_accuracy: 0.9797\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0616 - sparse_categorical_accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eade46dbe0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
    "                # Set weight \"2\" for class \"5\",\n",
    "                # making this class 2x more important\n",
    "                5: 2.,\n",
    "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
    "print('Fit with class weight')\n",
    "model.fit(x_train, y_train,\n",
    "          class_weight=class_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit with sample weight\n",
      "Train on 50000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 2s 34us/sample - loss: 0.3705 - sparse_categorical_accuracy: 0.9010\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.1843 - sparse_categorical_accuracy: 0.9479\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.1332 - sparse_categorical_accuracy: 0.9628\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 1s 25us/sample - loss: 0.1069 - sparse_categorical_accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eadaf20748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the same example using `sample_weight` instead:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "print('\\nFit with sample weight')\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          sample_weight=sample_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3751 - sparse_categorical_accuracy: 0.9034\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1743 - sparse_categorical_accuracy: 0.9523\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eadfaa18d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passing data to multi-input, multi-output models**\n",
    "\n",
    "上面的例子中，输入和输出都是一个，实际上，输入和输出可以是多个。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 8ms/sample - loss: 17.7147 - score_output_loss: 6.6453 - class_output_loss: 4.1911 - score_output_mean_absolute_percentage_error: 778.1131 - score_output_mean_absolute_error: 2.5221 - class_output_categorical_accuracy: 0.1500\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 270us/sample - loss: 13.2868 - score_output_loss: 3.9830 - class_output_loss: 4.4634 - score_output_mean_absolute_percentage_error: 616.2286 - score_output_mean_absolute_error: 2.0434 - class_output_categorical_accuracy: 0.1400\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 270us/sample - loss: 11.0805 - score_output_loss: 2.9004 - class_output_loss: 4.7994 - score_output_mean_absolute_percentage_error: 511.3189 - score_output_mean_absolute_error: 1.7522 - class_output_categorical_accuracy: 0.1500\n",
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 0s 290us/sample - loss: 9.5105 - score_output_loss: 2.5554 - class_output_loss: 4.7654 - score_output_mean_absolute_percentage_error: 427.7742 - score_output_mean_absolute_error: 1.5098 - class_output_categorical_accuracy: 0.1600\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 250us/sample - loss: 8.2270 - score_output_loss: 1.5756 - class_output_loss: 4.7369 - score_output_mean_absolute_percentage_error: 347.2895 - score_output_mean_absolute_error: 1.2764 - class_output_categorical_accuracy: 0.1800\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 260us/sample - loss: 7.3491 - score_output_loss: 1.2593 - class_output_loss: 4.6348 - score_output_mean_absolute_percentage_error: 281.6984 - score_output_mean_absolute_error: 1.0895 - class_output_categorical_accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eaed52bc50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "image_input = keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = keras.Input(shape=(None, 10), name='ts_input')\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name='score_output')(x)\n",
    "class_output = layers.Dense(5, name='class_output')(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy(from_logits=True)},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weights={'score_output': 2., 'class_output': 1.})\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using callbacks**\n",
    "\n",
    "Keras 中的 Callbacks 能在训练中被调用（如在 epoch 开始时，在 batch 结束时，在 epoch 结束时）等，可以实现如下功能：\n",
    "* 训练过程中执行验证\n",
    "* 满足条件时保存 checkpoint\n",
    "* 当训练平稳时改变学习率\n",
    "* 当训练平稳时对 top layers 执行 fine-tuning\n",
    "* 训练结束或某性能阈值到达时发送邮件或其他消息提示\n",
    "\n",
    "Callbacks 可以作为 list 传入 fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.3824 - sparse_categorical_accuracy: 0.8924 - val_loss: 0.2345 - val_sparse_categorical_accuracy: 0.9315\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.1767 - sparse_categorical_accuracy: 0.9470 - val_loss: 0.1772 - val_sparse_categorical_accuracy: 0.9456\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 1s 26us/sample - loss: 0.1278 - sparse_categorical_accuracy: 0.9629 - val_loss: 0.1746 - val_sparse_categorical_accuracy: 0.9492\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 1s 26us/sample - loss: 0.0993 - sparse_categorical_accuracy: 0.9702 - val_loss: 0.1428 - val_sparse_categorical_accuracy: 0.9587\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 1s 26us/sample - loss: 0.0828 - sparse_categorical_accuracy: 0.9751 - val_loss: 0.1467 - val_sparse_categorical_accuracy: 0.9581\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.0705 - sparse_categorical_accuracy: 0.9786 - val_loss: 0.1414 - val_sparse_categorical_accuracy: 0.9597\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eaed6d82b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内置的 callbacks 为：\n",
    "* ModelCheckpoint: 保存模型\n",
    "* EarlyStopping: 验证 metrics 不增长时结束训练\n",
    "* TensorBoard: 保存模型 logs，可以在 TensorBoard 中可视化\n",
    "* CSVLogger: streams loss and metrics data to a CSV file.\n",
    "\n",
    "**Writing your own callback**\n",
    "\n",
    "可以通过继承 keras.callbacks.Callback 来定制自己的 callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpointing models**\n",
    "\n",
    "当数据量比较大时，保存模型的 checkpoints 很重要，最简单的方式是使用 ModelCheckpoint callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.3679 - sparse_categorical_accuracy: 0.8968\n",
      "Epoch 00001: val_loss improved from inf to 0.22211, saving model to mymodel_1\n",
      "WARNING:tensorflow:From C:\\Users\\kazh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: mymodel_1\\assets\n",
      "40000/40000 [==============================] - 2s 51us/sample - loss: 0.3661 - sparse_categorical_accuracy: 0.8972 - val_loss: 0.2221 - val_sparse_categorical_accuracy: 0.9349\n",
      "Epoch 2/3\n",
      "38784/40000 [============================>.] - ETA: 0s - loss: 0.1656 - sparse_categorical_accuracy: 0.9513\n",
      "Epoch 00002: val_loss improved from 0.22211 to 0.16985, saving model to mymodel_2\n",
      "INFO:tensorflow:Assets written to: mymodel_2\\assets\n",
      "40000/40000 [==============================] - 2s 43us/sample - loss: 0.1655 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.1699 - val_sparse_categorical_accuracy: 0.9502\n",
      "Epoch 3/3\n",
      "39168/40000 [============================>.] - ETA: 0s - loss: 0.1197 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 00003: val_loss improved from 0.16985 to 0.15109, saving model to mymodel_3\n",
      "INFO:tensorflow:Assets written to: mymodel_3\\assets\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.1195 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.1511 - val_sparse_categorical_accuracy: 0.9567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eaeee4a4a8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using learning rate schedules**\n",
    "\n",
    "训练过程中逐渐减小学习率，即学习率衰减，实现机制可以是静态的（fixed in advance, as a function of the current epoch or the current batch index），也可以是动态的（responding to the current behavior of the model, in particular the validation loss）。\n",
    "\n",
    "可以通过传 learning_rate 来实现静态学习率衰减："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内置的学习率 callback 为 ReduceLROnPlateau callback。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Writing your own training & evaluation loops from scratch\n",
    "\n",
    "**Using the GradientTape: a first end-to-end example**\n",
    "\n",
    "在 GradientTape scope 内调用模型可以得到训练权重的梯度，使用优化器，就可以更新这些权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.4334452152252197\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.2208168506622314\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.164945125579834\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.1104955673217773\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.126343011856079\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.987076997756958\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.848045825958252\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.8498668670654297\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.7770605087280273\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.5975961685180664\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.6125603914260864\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.3932198286056519\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "            \n",
    "            # Compute the loss value for this minibatch\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low-level handling of metrics**\n",
    "\n",
    "加入 metrics 的流程为：\n",
    "* 在 loop 开始出实例化 metric\n",
    "* 每个 batch 后调用 metric.update_state()\n",
    "* 需要展示 metric 的值时调用 metric.result()\n",
    "* 清除 metric 状态时调用 metric.reset_states() (通常在 epoch 结束时)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.3310351371765137\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.230015277862549\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.2347941398620605\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.1749868392944336\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.22519999742507935\n",
      "Validation acc: 0.41119998693466187\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.988825798034668\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.9822006225585938\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.8704516887664795\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.831594705581665\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.5037800073623657\n",
      "Validation acc: 0.6004999876022339\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.7841203212738037\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.6937618255615234\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.5491985082626343\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.4165323972702026\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.6460400223731995\n",
      "Validation acc: 0.7210999727249146\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update training metric.\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "        \n",
    "        # Log every 200 batches\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "            \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "    # Run a validation loop at the end of each epoch\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        # Update val metrics\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low-level handling of extra losses**\n",
    "\n",
    "在前面的小节有个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当调用模型时，前向过程产生的 losses 会被加入到 model.losses 属性中，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=693212, shape=(), dtype=float32, numpy=6521.1426>]\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train)\n",
    "print(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次调用模型，保存的是最后一次的 losses，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=693269, shape=(), dtype=float32, numpy=8.187633>]\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train[:64])\n",
    "logits = model(x_train[64: 128])\n",
    "logits = model(x_train[128: 192])\n",
    "print(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要保存所有这些 loss，需要做的就是加上 sum(model.losses)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 10.775253295898438\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.5282602310180664\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.374560594558716\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3456227779388428\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.336315870285034\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.326626777648926\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.32987904548645\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3131821155548096\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 2.308389663696289\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.3308935165405273\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.3269221782684326\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3181331157684326\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "            # Add extra losses created during this forward pass:\n",
    "            loss_value += sum(model.losses)\n",
    "            \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Log every 200 batches:\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
