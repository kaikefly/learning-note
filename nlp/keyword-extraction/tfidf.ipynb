{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "tf-idf (term frequency-inverse document frequency) 是一种统计方法，用以评估一字词对于语料库中一份文件的重要程度。某个子词的重要性会随着它在文件中出现的次数成正比增加，但同时会随着它在语料中出现的频率成反比下降。\n",
    "## tf\n",
    "在一份给定的文件里，tf 指的是某个给定的词在该文件中出现的频率，对于在某一特定文件里的词语 $t_i$ 来说，它的重要性表示为：\n",
    "$$tf_{i,j} = \\frac{n_{i,j}}{\\sum_{k}n_{k,j}}$$\n",
    "其中，$n_{i,j}$ 是该词在文件 $d_j$ 中出现的次数，分母是文件 $d_j$ 中所有字词的出现次数之和。\n",
    "## idf\n",
    "idf 是一个词语普遍重要性的度量，某一特定词语的 idf 可有总文件数目除以包含该词语的文件的数目，再将结果取以10为底的对数得到：\n",
    "$$idf_i = log{\\frac{|D|}{|\\{j:t_i \\in d_j\\}|}}$$\n",
    "其中\n",
    "* $|D|$: 语料库中的文件总数\n",
    "* $|\\{j:t_i \\in d_j\\}|$: 包含词语 $t_i$ 的文件数目，如果词语不在数据中，就导致分母为零，可使用 $1+|\\{j:t_i \\in d_j\\}|$。\n",
    "\n",
    "然后 $tfidf_{i,j} = tf_{i,j} \\cdot idf_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "\n",
    "text1 = \"\"\"\n",
    "If you like tuna and tomato sauce- try combining the two.\n",
    "It's really not as bad as it sounds.\n",
    "If the Easter Bunny and the Tooth Fairy had babies would they take\n",
    "your teeth and leave chocolate for you?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1, 'TFIDF_score': 0.036860464373469494, 'key': 'if'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.036860464373469494, 'key': 'you'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'like'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'tuna'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.036860464373469494, 'key': 'and'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'tomato'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'sauce'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'try'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'combining'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.036860464373469494, 'key': 'the'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.09987384442437362, 'key': 'two'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.13732653608351372, 'key': 'its'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.13732653608351372, 'key': 'really'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.13732653608351372, 'key': 'not'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.27465307216702745, 'key': 'as'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.13732653608351372, 'key': 'bad'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.13732653608351372, 'key': 'it'},\n",
       " {'doc_id': 2, 'TFIDF_score': 0.13732653608351372, 'key': 'sounds'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.02027325540540822, 'key': 'if'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.04054651081081644, 'key': 'the'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'easter'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'bunny'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.04054651081081644, 'key': 'and'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'tooth'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'fairy'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'had'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'babies'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'would'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'they'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'take'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'your'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'teeth'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'leave'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'chocolate'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.054930614433405495, 'key': 'for'},\n",
       " {'doc_id': 3, 'TFIDF_score': 0.02027325540540822, 'key': 'you'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_string_special_characters(s):\n",
    "    \"\"\"\n",
    "    This function removes special characters from within a string\n",
    "    \"\"\"\n",
    "    # Replace special character with ' '\n",
    "    stripped = re.sub('[^\\w\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "    \n",
    "    # Change any whitespace to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "    \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    \n",
    "    return stripped\n",
    "\n",
    "def get_doc(text_sents_clean):\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1\n",
    "        count = count_words(sent)\n",
    "        temp = {'doc_id': i, 'doc_length': count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info\n",
    "\n",
    "def count_words(sent):\n",
    "    return len(word_tokenize(sent))\n",
    "\n",
    "def create_freq_dict(sents):\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            freq_dict[word] = freq_dict.get(word, 0) + 1\n",
    "        temp = {'doc_id': i, 'freq_dict': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "\n",
    "    return freqDict_list\n",
    "\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "    TF_scores = []\n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp = {'doc_id': id,\n",
    "                   'TF_score': tempDict['freq_dict'][k] / doc_info[id-1]['doc_length'],\n",
    "                   'key': k}\n",
    "            TF_scores.append(temp)\n",
    "\n",
    "    return TF_scores\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id': counter, 'IDF_score': math.log(len(doc_info) / count), 'key': k}\n",
    "            IDF_scores.append(temp)\n",
    "\n",
    "    return IDF_scores\n",
    "\n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id': j['doc_id'],\n",
    "                       'TFIDF_score': j['IDF_score'] * i['TF_score'],\n",
    "                       'key': i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "        \n",
    "    return TFIDF_scores\n",
    "\n",
    "text_sents = sent_tokenize(text1)\n",
    "text_sents_clean = [remove_string_special_characters(s) for s in text_sents]\n",
    "doc_info = get_doc(text_sents_clean)\n",
    "\n",
    "freqDict_list = create_freq_dict(text_sents_clean)\n",
    "TF_scores = computeTF(doc_info, freqDict_list)\n",
    "IDF_scores = computeIDF(doc_info, freqDict_list)\n",
    "TFIDF_scores = computeTFIDF(TF_scores, IDF_scores)\n",
    "TFIDF_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
