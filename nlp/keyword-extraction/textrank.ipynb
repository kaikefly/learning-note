{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank\n",
    "\n",
    "其它关键词提取模型都要基于一个现成的语料库，如 tf-idf 需要统计每个词在语料库中的多少个文档中出现过，也就是逆文档频率；主题模型的关键词提取算法则是要通过对大规模文档的学习，来发现文档的隐含主题。而 TextRank 算法不需要语料库，仅对单文档进行分析即可提取文档的关键词。TextRank 的基本思想来源于 PageRank 算法，其基本思想有两条：\n",
    "* 链接数量。一个网页被越多的其他网页链接，说明这个网页越重要。\n",
    "* 链接质量。一个网页被一个越高权重的网页链接，也能表面这个网页越重要。\n",
    "所以，一个网页的 PageRank 值计算公式为：\n",
    "\n",
    "$In(V_i)$ 为 $V_i$ 的入链集合，$Out(V_j)$ 为 $V_j$ 的出链集合，$|Out(V_j)|$ 是出链的数量，因为每个网页要将它自身的分数平均地贡献给每个出链，则 $\\frac{1}{|Out(V_j)|}\\cdot{S(V_j)}$ 即为 $V_j$ 贡献给 $V_i$ 的分数。将 $V_i$ 的所有入链贡献给它的分数加起来，就是 $V_i$ 自身的得分。\n",
    "$$S(V_i) = \\sum_{j\\in In(V_i)}\\left(\\frac{1}{|Out(V_j)|}\\cdot{S(V_j)}\\right)$$\n",
    "算法开始时会将所有网页的得分初始化为1，然后通过多次迭代来对每个网页的分数进行收敛，收敛时的得分就是网页的最终得分。\n",
    "\n",
    "为了处理孤立网页，对公式加入阻尼系数 $d$，改造后的计算公式为：\n",
    "$$S(V_i) = (1-d) + d\\cdot\\sum_{j\\in In(V_i)}\\left(\\frac{1}{|Out(V_j)|}\\cdot{S(V_j)}\\right)$$\n",
    "PageRank 是有向无权图，而 TextRank 进行自动摘要是有权图，在计算分数时，除了要考虑链接句的重要性外，还要考虑句子间的相似性，所以 TextRank 的表达式：\n",
    "$$WS(V_i) = (1-d) + d\\cdot\\sum_{V_j\\in In(V_i)}\\left(\\frac{w_{ji}}{\\sum_{V_k\\in Out(V_j)w_{jk}}}\\cdot{WS(V_j)}\\right)$$\n",
    "当 TextRank 应用到关键词抽取时，与应用在自动摘要时有两点不同：1）词与词之间的关联没有权重，2）每个词不是与文档中所有词都有连接。\n",
    "根据第一点，TextRank 中的分数计算公式退化为与 PageRank 一致，将得分平均贡献给每个链接的词：\n",
    "$$WS(V_i) = (1-d) + d\\cdot\\sum_{j\\in In(V_i)}\\left(\\frac{1}{|Out(V_j)|}\\cdot{WS(V_j)}\\right)$$\n",
    "根据第二点，TextRank 应用在关键词提取时，使用了“窗口“的概念，在窗口内的词相互间都有链接关系。如以下文本：\n",
    "\n",
    "\"世界献血日，学校团体、献血服务志愿者等可到血液中心参观检验加工过程。我们会对检验结果进行公示，同时血液的价格也将进行公示。\"\n",
    "\n",
    "将窗口大小设为5，可得到以下几个窗口：\n",
    "\n",
    "1 [世界，献血，日，学校，团体]\n",
    "\n",
    "2 [献血，日，学校，团体，献血]\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/z8/fp_nd09x3qzc50dxl2p2_50c0000gn/T/jieba.cache\n",
      "Loading model cost 0.918 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['城市', '爱心', '救助', '中国', '社会', '晋江市', '基金会', '大会', '介绍', '公益活动']\n"
     ]
    }
   ],
   "source": [
    "# textrank for keyword extraction\n",
    "\n",
    "import jieba.posseg\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "\n",
    "STOP_WORDS = set((\n",
    "        \"the\", \"of\", \"is\", \"and\", \"to\", \"in\", \"that\", \"we\", \"for\", \"an\", \"are\",\n",
    "        \"by\", \"be\", \"as\", \"on\", \"with\", \"can\", \"if\", \"from\", \"which\", \"you\", \"it\",\n",
    "        \"this\", \"then\", \"at\", \"have\", \"all\", \"not\", \"one\", \"has\", \"or\", \"that\"\n",
    "    ))\n",
    "\n",
    "\n",
    "class UndirectWeightedGraph:\n",
    "    d = 0.85\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "       \n",
    "    def addEdge(self, start, end, weight):\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    "        \n",
    "    def rank(self):\n",
    "        ws = defaultdict(float)\n",
    "        outSum = defaultdict(float)\n",
    "        \n",
    "        wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "        for n, out in self.graph.items():\n",
    "            ws[n] = wsdef\n",
    "            outSum[n] = sum((e[2] for e in out), 0.0)\n",
    "        \n",
    "        sorted_keys = sorted(self.graph.keys())\n",
    "        for x in range(10):\n",
    "            for n in sorted_keys:\n",
    "                s = 0\n",
    "                for e in self.graph[n]:\n",
    "                    s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                ws[n] = (1 - self.d) + self.d * s\n",
    "                \n",
    "        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    "        \n",
    "        for w in ws.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "                \n",
    "        for n, w in ws.items():\n",
    "            # to unify the weights, don't *100.\n",
    "            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    "\n",
    "        return ws\n",
    "\n",
    "        \n",
    "class TextRank(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = self.postokenizer = jieba.posseg.dt\n",
    "        self.stop_words = STOP_WORDS.copy()\n",
    "        self.pos_filt = frozenset(('ns', 'n', 'vn', 'v'))\n",
    "        self.span = 5\n",
    "        \n",
    "    def pairfilter(self, wp):\n",
    "        return (wp.flag in self.pos_filt and len(wp.word.strip()) >= 2\n",
    "                and wp.word.lower() not in self.stop_words)\n",
    "    \n",
    "    def textrank(self, sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False):\n",
    "        self.pos_filt = frozenset(allowPOS)\n",
    "        g = UndirectWeightedGraph()\n",
    "        cm = defaultdict(int)\n",
    "        words = tuple(self.tokenizer.cut(sentence))\n",
    "        for i, wp in enumerate(words):\n",
    "            if self.pairfilter(wp):\n",
    "                for j in range(i + 1, i + self.span):\n",
    "                    if j >= len(words):\n",
    "                        break\n",
    "                    if not self.pairfilter(words[j]):\n",
    "                        continue\n",
    "                    if allowPOS and withFlag:\n",
    "                        cm[(wp, words[j])] += 1\n",
    "                    else:\n",
    "                        cm[(wp.word, words[j].word)] += 1\n",
    "                        \n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        if withWeight:\n",
    "            tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)\n",
    "        else:\n",
    "            tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)\n",
    "\n",
    "        if topK:\n",
    "            return tags[:topK]\n",
    "        else:\n",
    "            return tags\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "           '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "           '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "           '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "           '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "           '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "           '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "           '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "           '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "           '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "           '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "    \n",
    "    tr = TextRank()\n",
    "    keywords = tr.textrank(text, 10)\n",
    "    print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
