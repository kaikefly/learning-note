{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 学习\n",
    "\n",
    "Transformer 是文章《Attention Is All You Need》提出的一个模型结构，是 NLP 领域非常有创新性的成果。基于 Transformer 构建的模型 Bert，更是在多个自然语言任务上都取得了革命性的效果，可见 Transformer 的厉害之处。\n",
    "\n",
    "Transformer 的创新之处是完全摒弃了循环和卷积的思路，只用到了注意力机制。这一架构的主要优点是层输出可以并行计算，而非像 RNN 这样的序列计算，而且能学习长距离的依赖。\n",
    "\n",
    "先看看 Transformer 的整体架构：\n",
    "<img src=\"/images/model_architecture.png\" width=\"700\">\n",
    "下面，一步步地剖析这个架构。\n",
    "\n",
    "### 1. Positional encoding (位置编码)\n",
    "\n",
    "由于 Transformer 不包括循环或卷积，所以为了标记单词在句子中的相对位置，模型使用了 positional encoding。positional encoding 被加到 word embedding 向量中作为最终的输入。加入位置编码后，每个词根据它们的语义相似度和在句子中的位置来决定它们的最终相似度。\n",
    "\n",
    "文章使用的位置编码为：\n",
    "<img src=\"/images/positional_encoding.png\" width=\"480\">\n",
    "\n",
    "其中，$pos$ 表示位置，$i$ 表示维度。这样，positinal encoding 的每一维度都对应一个正弦曲线。为什么选择这个函数，作者认为这样可以让模型更容易地通过相对位置来学习。文章也尝试用学习好的位置编码，发现这两种方法效果差不多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2))/np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # 偶数 index 上用 sin 函数\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # 奇数 index 上用 cos 函数\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 512)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(100, 512)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Masking\n",
    "\n",
    "Masking 的作用是 mask 序列中的填充位，确保模型不会将填充位作为输入。mask 值表明了填充值 0 出现的位置，在这些位置 mask 输出 1，否则输出 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9, shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look-ahead mask 用来 mask 序列中未来的 token。这意味着，为了预测第三个词，只能使用第一、二个词，为了预测第四个词，只能使用前三个词等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention\n",
    "\n",
    "Transformer 有两种 Attention：Scaled dot-product attention 和 Multi-head attention，其实 scaled dot-product attention 是 multi-head attention 的组成部分。\n",
    "<img src=\"/images/transformer_attention.png\" width=530>\n",
    "Attention 的作用就是把 query、key-value对 映射到 output 中，它们都是向量。ouput 是 value 的加权和，权重通过 query 和对应的 key 计算得到。\n",
    "\n",
    "**Scaled dot-product attention**\n",
    "\n",
    "按比缩放点积注意力的输入为 queries、$d_k$ 维度的 keys 和 $d_v$ 维度的 values。先计算 query 和 keys 的乘积，除以 $\\sqrt{d_k}$，然后求 softmax 操作得到 values 的权重。实际操作中，attention 操作是在一组 queries 上同时进行的，堆叠在一起形成矩阵 $Q$，keys 和 values 同样堆叠起来形成矩阵 $K$ 和 $V$。计算方式如下：\n",
    "$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "这里除以 $\\sqrt{d_k}$ 是因为当 $d_k$ 比较大时，点积也会变大，这样 softmax 出来后会得到很小的梯度，所以除以 $\\sqrt{d_k}$ 来规避这种影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    这里 q, k, v 的维度要满足一定的条件：\n",
    "    q, k, v 必须具有匹配的 leading dimensions\n",
    "    k, v 要满足 seq_len_k == seq_len_v\n",
    "    Args:\n",
    "        q: query shape (..., seq_len_q, depth)\n",
    "        k: key shape (..., seq_len_k, depth)\n",
    "        v: value shape (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor，其维度要能转换为 (..., seq_len_q, seq_len_k)。默认为 None\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # 对 k 进行转置，结果为 (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # 在最后一个轴上做 softmax\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output 代表 attention 权重和 value 的乘积，这能保证关注的词被保留，而无关的词被清除掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
    "    print('Attention weights are:', temp_attn)\n",
    "    print('Output is:', temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tf.Tensor(\n",
      "[[0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is: tf.Tensor(\n",
      "[[ 10.    0. ]\n",
      " [550.    5.5]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
    "# 第一个 query [0, 10, 0] 符合第二个 key [0, 10, 0], 因此返回了第二个 value [10, 0]\n",
    "# 第二个 query[0, 0, 10] 符合第三、四个 key [0, 0, 10]，因此返回 第三、四个 value 的平均值\n",
    "# 第三个 query [10, 10, 0] 符合第一、二个 key [10, 0, 0] [0, 10, 0]，因此返回它们的平均值\n",
    "temp_q = tf.constant([[0, 10, 0], [0, 0, 10], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-head attention**\n",
    "\n",
    "多头注意力不是简单的执行 $d_{model}$ 维的 attention 操作，而是分别对 queries、keys 和 values 做了线性映射（这三个映射是不同的），分别映射到 $d_k$、$d_k$ 和 $d_v$ 维度。然后在每个映射版本上并行地执行 attention 操作，生成 $d_v$ 的输出值，然后将这些值拼接起来再进行一次线性映射得到最终结果（如上图所示），这么做的意义是模型可以注意来自不同表示空间的不同位置的信息。\n",
    "<img src=\"/images/multihead_attention.png\" width=450>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads  # 8\n",
    "        self.d_model = d_model  # 512\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads  # 64\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_head(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        先将最后一维分割为 (num_heads, depth),\n",
    "        然后做 transpose，变为（batch_size, num_heads, seq_len, detph）\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  # (batch_size, seq_len, num_heads, depth)\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_len, depth)\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_head(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_head(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_head(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在序列的每个位置，MultiHeadAttention 在序列中的所有其它位置运行所有 8 个注意力头，在每个位置，返回一个新的同样长度的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512)\n",
      "(1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "print(out.shape)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Position-wise Feed-forward networks\n",
    "\n",
    "除了 attention 层，encoder 和 decoder 的每层还有全连接前向网络，包括两层线性变换，两层之间是 ReLU 激活函数。\n",
    "$$FFN(x)=max(0, xW_1+b_1)W_2+b_2$$\n",
    "线性变换在不同位置之间是相同的，层与层之间是不同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):  # dff: dimension of feed forward\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "print(sample_ffn(tf.random.uniform((64, 50, 512))).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Encoder and decoder\n",
    "\n",
    "Transformer 模型与标准的具有注意力机制的 seq2seq 模型，具有一样的模式：\n",
    "* 输入语句经过 N 个 encoder 层，为序列中每个词生成一个输出；\n",
    "* decoder 关注 encoder 的输出以及它自身的输入（自注意力）来预测下一个词。\n",
    "\n",
    "**Encoder layer**\n",
    "\n",
    "每个 encoder 层包含两个子层：\n",
    "* Multi-head attention (with padding mask)\n",
    "* Position-wise feed forward networks\n",
    "\n",
    "两层之间用残差连接，然后后面是 layer normalization，残差连接可以避免梯度消失问题。Transformer 中有 N 个 encoder 层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "print(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder layer**\n",
    "\n",
    "每个 decoder 层有三个子层:\n",
    "* Masked multi-head attention (with look ahead mask and padding mask)\n",
    "* MultiheadAttention (with padding mask)，其中 V(value) 和 K(key) 以 encoder 的输出为输入，Q(query) 接受 masked multi-head attention 子层的输出。\n",
    "* Position-wise feed forward networks\n",
    "\n",
    "同 encoder 层一样，两子层之间是残差连接，后接一个 layer normalization。Transformer 有 N 个 decoder 层。\n",
    "\n",
    "当 Q 接收到 decoder 第一个子层的输出，K 接收到 encoder 的输出时，attention weights 表示根据 encoder 的输出而赋予 decoder 的输入的重要程度。换句话说，decoder 根据 encoder 的输出和自身输出的自注意力来预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_out.shape == (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "print(sample_decoder_layer_output.shape)  # (batch_size, target_seq_len, d_model)，其中 sample_encoder_layer_output 为 (64, 43, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder (编码器)**\n",
    "\n",
    "Encoder 包括三个部分：\n",
    "* Input embedding\n",
    "* Positional encoding\n",
    "* N encoder layers\n",
    "\n",
    "输入首先经过 embedding，然后与 positional encoding 相加，相加的结果作为 encoder 的输入，encoder 的输出作为 decoder 的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)  # (1, maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]  # 第二维为序列长度\n",
    "        \n",
    "        # adding embedding and position encoding\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # 根据论文，embedding 的权重要乘以 sqrt(d_model)\n",
    "        x += self.pos_encoding[:, :seq_len, :]  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                        dff=2048, input_vocab_size=8500, maximum_position_encoding=10000)\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), training=False, mask=None)\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder (解码器)**\n",
    "\n",
    "decoder 也包括三个部分：\n",
    "* Output embedding\n",
    "* Positional encoding\n",
    "* N decoder layers\n",
    "\n",
    "目标（target）经过 word embedding 后，和位置编码相加。相加的结果作为解码器的输入，解码器的输出作为最后线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                          for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "            \n",
    "        # x.shape == (batch, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 26, 512)\n",
      "(64, 8, 26, 62)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                        dff=2048, target_vocab_size=8000, maximum_position_encoding=5000)\n",
    "output, attn = sample_decoder(tf.random.uniform((64, 26)),\n",
    "                             enc_output = sample_encoder_output,\n",
    "                             training=False, look_ahead_mask=None, padding_mask=None)\n",
    "print(output.shape)  # (batch_size, target_seq_len, d_model)\n",
    "print(attn['decoder_layer2_block2'].shape)  # (batch_size, num_heads, seq_len_q, seq_len_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.创建 Transformer\n",
    "\n",
    "Transformer 包括编码器、解码器和最后的线性层，解码器的输出是线性层的输入，返回线性层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                              input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                              target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)  # 最后的线性层，输出的维度为 target_vocab_size\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask,\n",
    "            look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)  # decoder 以 encoder 的输出作为输入\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 26, 8000)\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=8000,\n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
    "                              enc_padding_mask=None,\n",
    "                              look_ahead_mask=None,\n",
    "                              dec_padding_mask=None)\n",
    "\n",
    "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建数据集**\n",
    "\n",
    "Transformer 的整体架构已经介绍清楚，下面以具体的例子来将整个过程串起来。首先使用 TFDS 来下载数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                              as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en, in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果某个词没在词典中出现过，tokenizer 会将其分割成 subwords 然后再编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ---> T\n",
      "1248 ---> ran\n",
      "7946 ---> s\n",
      "7194 ---> former \n",
      "13 ---> is \n",
      "2799 ---> awesome\n",
      "7877 ---> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print('{} ---> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    \"\"\"\n",
    "    Add a start and end token to the input and target.\n",
    "    \"\"\"\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "        lang1.numpy()) + [tokenizer_pt.vocab_size + 1]\n",
    "    \n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "        lang2.numpy()) + [tokenizer_en.vocab_size + 1]\n",
    "    \n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使模型小巧训练快，去掉长度大于40的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# Cache the dataset to memory to get a speedup while reading from it\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n",
    "    BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(\n",
    "    BATCH_SIZE, padded_shapes=([-1], [-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=316814, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8214, 1259,    5, ...,    0,    0,    0],\n",
       "        [8214,  299,   13, ...,    0,    0,    0],\n",
       "        [8214,   59,    8, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,   95,    3, ...,    0,    0,    0],\n",
       "        [8214, 5157,    1, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: id=316815, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   18,   12, ...,    0,    0,    0],\n",
       "        [8087,  634,   30, ...,    0,    0,    0],\n",
       "        [8087,   16,   13, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   17, 4981, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**配置超参数（hyperparameters）**\n",
    "\n",
    "Transformer 的基础模型使用的数值为：num_layers=6, d_model=512, dff=2048。在本示例中，为了让模型训练速度加快，使用如下较小的值。\n",
    "\n",
    "Note: 通过改变一下的数值，可以获得在许多任务上达到最先进水平的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2  # 加了 SOS、EOS\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2  # 加了 SOS、EOS\n",
    "\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8216, 8089)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size, target_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**优化器（Optimizer）**\n",
    "\n",
    "论文使用 Adam 优化器：$\\beta_1=0.9, \\beta_2=0.98, \\epsilon=10^{-9}$，并且使用可变学习率：\n",
    "$$lrate=d^{-0.5}_{model}\\cdot min(step\\_num^{-0.5}, step\\_num\\cdot warmup\\_steps^{-1.5})$$\n",
    "学习率在第一个 *warmup_steps* 训练轮内线性增长，然后减小。论文中使用 $warmup_steps=4000$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)  # rsqrt() 表示求平方根的倒数\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train step')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)).numpy())\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xlabel('Train step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**损失函数与指标（Loss and metrics）**\n",
    "\n",
    "由于目标序列是填充（padded）过的，因此在计算损失时应使用 padding mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and checkpointing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                        input_vocab_size, target_vocab_size,\n",
    "                        pe_input=input_vocab_size,\n",
    "                        pe_target=target_vocab_size,\n",
    "                        rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # Used in the 2nd attention block in the decoder\n",
    "    # This padding mask is used to mask the encoder outputs\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # Used in the 1st attention block in the decoder\n",
    "    # It is used to pad and mask future tokens in the input received by the decoder\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])  # size 为 target_seq_len\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                          optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the lastest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 target 分成 tar_input 和 tar_real。tar_input 作为 decoder 的输入，tar_real 是一样的输入，只是位置左移一位。在 tar_input 的每一位置，tar_real 对应应该被预测的下一个 token。\n",
    "\n",
    "例如，sentence = 'SOS A lion in the jungle is sleeping EOS'\n",
    "```\n",
    "tar_input = 'SOS A lion in the jungle is sleeping'\n",
    "tar_real = 'A lion in the jugle is sleeping EOS'\n",
    "```\n",
    "\n",
    "Transformer 是 auto-regressive 模型，它一次预测一部分，然后以当前输出来决定下一步要做什么。在训练过程中，本示例使用了 teacher-forcing 方法，无论模型在当前时刻预测出什么，teacher-forcing 方法都会将真实的输出传递到下一个时间步骤上。\n",
    "\n",
    "Transformer 预测每个词的时候，self-attention 可以查看输入序列的前面的词，来更好地预测下一个词。为了防止 decoder 偷看答案，要使用 look-ahead mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]  # EOS 之前的词\n",
    "    tar_real = tar[:, 1:]  # SOS 之后的词\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp,\n",
    "                                    True,\n",
    "                                    enc_padding_mask,\n",
    "                                    combined_mask,\n",
    "                                    dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                         train_loss.result(),train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate\n",
    "\n",
    "Evaluate 阶段有以下步骤：\n",
    "* 使用 tokenizer_pt 将输入序列编码，并加上 start 和 end token（和模型训练时一样）。这是 encoder 输入；\n",
    "* decoder 输入为 start token == tokenizer_en.vocab_size;\n",
    "* 计算 padding mask 和 look ahead mask。\n",
    "* decoder 通过查看 encoder 输出和它自身输出（自注意力）来预测结果；\n",
    "* 选择最后一个词，计算它的 argmax;\n",
    "* 将预测的词拼接到 decoder 输入，然后传递给 decoder;\n",
    "* 在这种方法中，decoder 根据它预测的之前一个词来预测下一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "    \n",
    "    # 输入语句是葡萄牙语，增加开始和结束标记\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)  # 维度变为 (1, inp_sentence.shape)\n",
    "    \n",
    "    # 目标是英语，输入 transformer 的第一个词应该是英语的开始标记\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)  # 维度是（1, 1）\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "        \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input,\n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "        \n",
    "        # 从 seq_len 维度选择最后一个词\n",
    "        predictions = predictions[:, -1:, :]  # (1, 1, vocab_size)\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)  # (1, 1)\n",
    "        \n",
    "        # 如果 predicted_id 等于结束标记，就返回结果\n",
    "        if predicted_id == tokenizer_en.vocab_size + 1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "        \n",
    "        # 将 predicted_id 与输出，作为 decoder 的输入传递到 decoder\n",
    "        output = tf.concat([output, predicted_id], axis=-1)  # (1, output.shape[-1] + 1)\n",
    "    \n",
    "    # 返回的 squeeze 后的 output 维度为 [output.shape[1]]\n",
    "    return tf.squeeze(output, axis=0), attention_weights        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "        # 画出注意力权重\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "    \n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "    \n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "    \n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "    \n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    \n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result if i < tokenizer_en.vocab_size])\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: this is a problem that we have to solve the u.s . . .s . here .this this this this is this this this this \n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my parents heard about this idea . . .ssssmy my my my my my my my my my \n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é o primeiro livro que eu fiz.\n",
      "Predicted translation: this is the first book that i did n't know what i was . . . is .this is this this this this this this this here . ''\n",
      "Real translation: this is the first book i've ever done.\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
