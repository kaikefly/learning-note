{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "### 1. 背景知识\n",
    "\n",
    "在深层神经网络训练中，由于网络上层参数的变化而引起每层输入数据分布随之变化，这导致了训练过程变慢，这种现象叫做“Internal Covariate Shift”。\n",
    "\n",
    "Batch Normalization 是解决该问题的有效方法，具体做法为：\n",
    "* 对每个特征单独做 normalization。\n",
    "* 在每个 mini-batch 上做 normalization。\n",
    "* 同时，加上线性变换操作。\n",
    "\n",
    "它的优点是可以加速训练，不需要很小心地初始化学习率等参数以及缓解过拟合。同时，也存在一些缺点：\n",
    "* 当 batch size 比较小的时候，均值和方差没有很好的统计意义；\n",
    "* 不适用于 RNN 等动态网络结构；\n",
    "* 不适用于训练数据集和测试数据集方差比较大的时候。\n",
    "\n",
    "Layer Normalization 可以解决上述问题。\n",
    "\n",
    "### 2. Layer normalization\n",
    "\n",
    "#### 2.1 MLP 中的 LN\n",
    "\n",
    "LN 独立于 batch size，根据样本的特征数做归一化。先看 MLP 中的 LN，设 $H$ 是一层中隐藏节点的数量，$l$ 是 MLP 的层数，我们可以计算 LN 的归一化统计量 $\\mu$ 和 $\\sigma$：\n",
    "$$\\mu^l=\\frac{1}{H}\\sum^H_{i=1}a^l_{i},\\ \\ \\ \\sigma^l=\\sqrt{\\frac{1}{H}\\sum^H_{i=1}(a^l_{i}-\\mu^l)^2}$$\n",
    "上面统计量的计算和样本数量没有关系，只取决于隐藏节点的数量，所以只要隐层节点的数量足够多，就能保证 LN 的归一化统计量具有代表性。通过 $\\mu^l$ 和 $\\sigma^l$ 可以得到归一化后的值 ${\\hat{a}}^l$：\n",
    "$$\\hat{a}^l=\\frac{a^l-\\mu^l}{(\\sigma^l)^2+\\sqrt{\\epsilon}}$$\n",
    "其中，$\\epsilon$ 是一个很小的数，防止除0。\n",
    "\n",
    "为保证归一化操作不破坏之前的信息，LN 也使用一组参数 $g$（增益）和 $b$（偏置），等同于 BN 中的 $\\gamma$ 和 $\\beta$。假设激活函数为 $f$，则最终 LN 的输出为：\n",
    "$$h^l=f(g^l \\odot \\hat{a}^l+b^l)$$\n",
    "合并上式，并忽略参数 $l$，可以得到：\n",
    "$$h=f(\\frac{g}{\\sqrt{\\sigma^2+\\epsilon}}\\odot(a-\\mu)+b)$$\n",
    "\n",
    "#### 2.2 RNN 中的 LN\n",
    "\n",
    "在 RNN 中，可以非常简单的在每个时间片中使用 LN，而且在任何时间片我们都能保证归一化统计量统计的是 H 个节点的信息。对于 RNN 的时刻 t 的节点，其输入是 t-1 时刻的隐层状态 $h^{t-1}$ 和 t 时刻的输入数据 $x^t$，可以表示为：\n",
    "$$a^t=W_{hh}h^{t-1}+W_{xh}x^t$$\n",
    "接着便可以在 $a^t$ 上采取和上面一致的归一化过程：\n",
    "$$h^t=f(\\frac{g}{\\sqrt{(\\sigma^t)^2+\\epsilon}}\\odot(a^t-\\mu^t)+b),\\ \\ \\mu^t=\\frac{1}{H}\\sum^H_{i=1}a^t_i,\\ \\ \\sigma^t=\\sqrt{\\frac{1}{H}\\sum^H_{i=1}(a^t_i-\\mu^t)^2}$$\n",
    "\n",
    "### 3. tensorflow 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.layers.layer_norm(\n",
    "    inputs,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    activation_fn=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    begin_norm_axis=1,\n",
    "    begin_params_axis=-1,\n",
    "    scope=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般只需要调 tf.contrib.layers.layer_norm(inputs) 就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
